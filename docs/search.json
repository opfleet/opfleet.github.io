[
  {
    "objectID": "posts/PerceptronPost/index.html",
    "href": "posts/PerceptronPost/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Abstract\nIn this blog post, I am aiming to construct an implementation of the perceptron algorithm and then test its efficacy on a synthetic set of linearly separable data. I will then use the popular Iris dataset that can be found on Kaggle as a prime example of a linearly separable and non-linearly separable dataset. The Iris dataset is composed of three labels, one of which is linearly separable from the rest of the data, and two which are not. I will use this characteristic to use the dataset as both a linearly separable and non-linearly separable set in my exploration.\nLater, I will implement the minibatch perceptron algorithm, so that I may experiment with various batch sizes. This method is distinct from the classical perceptron algorithm, as the classical algorithm performs an update step on a single point in the data, while the minibatch algorithm may use multiple data points in each update step.\nHere is a link to my perceptron.py file, which contains my implementation of the perceptron algorithm that I use throughout this blog post.\nHere is my implementation of the perceptron.grad() function:\ndef grad(self, X, y):\n        s = self.score(X)\n        return torch.where(s*y &lt; 0, y @ X, 0.0)\nEach line corresponds to the two main lines of math in the perceptron algorithm: \\[s_i = w \\cdot x_i \\] and return the vector \\[ \\mathbb{1}[s_iy_i &lt; 0]y_ix_i\\]\nThe first line is fairly self-explanatory; my perceptron.score() function takes in the input x_i and returns its dot product against the weight value w, as defined by the algorithm. For the second line, the combination of the double-struck one and the condition it is multiplied against are the mathematical equivalent to an if statement, in that if the condition is true, multiply the following expression by 1, and if the condition is false, multiply the following expression by 0. Thus, I was able to use a torch.where() function to implement this equation, as it uses a condition to assign different values, depending on the evaluation of the condition.\n\n\nPart A: Implement Perceptron\nIn this section, I will be utilizing my perceptron algorithm contained in the file perceptron.py on arbitrary linearly-separable data, to ensure that my implementation works properly.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nHere is a visualization of my linearly-separable data, used to check my perceptron implementation.\n\nimport torch\nfrom matplotlib import pyplot as plt\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n    y = y.type(torch.FloatTensor)\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nHere I am running my perceptron algorithm on the data, and printing the loss at each iteration. We can see how the loss begins at 0.5, and decreases until it reaches 0, where my algorithm has terminated upon finding a separating line between the two classes.\n\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0.0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nprint(loss_vec)\n\n[tensor(0.5000), tensor(0.5000), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.0900), tensor(0.2600), tensor(0.2600), tensor(0.1700), tensor(0.1700), tensor(0.1700), tensor(0.1700), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.0033), tensor(0.3167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.0167), tensor(0.)]\n\n\nHere is a visualization showing the loss at each iteration of the perceptron update.\n\nplt.figure(figsize= (8,8))\nplt.plot(loss_vec, color = \"slategrey\")\n\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Perceptron Loss on Linearly Separable Data\")\n\n\n\n\n\n\n\n\n\n\nPart B: Experiments\nIn this section, I will be using the famous linearly-separable Iris data set from R.A. Fisher’s 1936 paper The Use of Multiple Measurements in Taxonomic Problems from the UCI Machine Learning Repository. This data set contains three iris species, each with 50 samples and multiple features for each data point. One species of iris is linearly separable from the other two. I will be illustrating the following claims concerning perceptron algorithms: 1. When using 2D data, if the data is linearly separable, then the perceptron will converge to weight vector w that describes a separating line. I will choose the setosa species of iris as a binary label, as it is linearly separable from the rest of the data. 2. When using 2D data, if the data is not linearly separable, the perceptron algorithm will not settle on a final value of w, but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy (assumption that 1000 iterations is sufficient). I will choose the versicolor species of iris as a binary label, as it is not linearly separable from the rest of the data. 3. The perceptron algorithm will be able to work in more than 2 dimensions of data. I will show this by running my algorithm on data with at least 5 features. While the Iris dataset only has 4 features, I will add a dummy 5th feature that is constant between every data point, thereby making the dataset 5th-dimensional, while still maintaining the linear separability of the dataset (when using the setosa species as a binary label).\nFirst, I will load the Iris dataset into the Jupyter Notebook.\n\nimport pandas as pd\ndf = pd.read_csv('Iris.csv')\nprint(df.head())\n\n   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n0   1            5.1           3.5            1.4           0.2  Iris-setosa\n1   2            4.9           3.0            1.4           0.2  Iris-setosa\n2   3            4.7           3.2            1.3           0.2  Iris-setosa\n3   4            4.6           3.1            1.5           0.2  Iris-setosa\n4   5            5.0           3.6            1.4           0.2  Iris-setosa\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nIn the following visualization, we can see that the Setosa species of Iris flower is linearly separable from the other two species (Versicolor and Virginica), when looking at the features Sepal Length and Petal Length. We can also see that using these two features, that the Versicolor and Virginica species are not linearly separable from one another. Since we are only using two features, we can consider this data to be 2 dimensional. Thus, we will try to prove our first two claims using the Sepal Length and Petal Length features of this dataset.\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nplot1 = sns.scatterplot(data= df, x= 'SepalLengthCm', y= 'PetalLengthCm', hue= 'Species', ax= ax)\nplot1.set(xlabel= 'Sepal Length (cm)', ylabel= 'Petal Length (cm)', title= \"Sepal Length vs. Petal Length\")\nplot1\n\n\n\n\n\n\n\n\nNow to prep our data. We will only be considering the features Sepal Length and Petal Length in our first two claims. Additionally, since we want to see whether one class is linearly separable from the other two, we will transform the ‘Species’ feature into a binary value representing whether the given Iris data point is of the Setosa species or not.\n\nx1 = torch.tensor(df['SepalLengthCm'].values)\nx2 = torch.tensor(df['PetalLengthCm'].values)\nbias = torch.ones(x1.shape[0])\nX = torch.stack((x1, x2, bias), -1)\n\nspecies_setosa = df['Species'] == 'Iris-setosa'\ny = torch.from_numpy(species_setosa.values)\n\nX = X.type(torch.FloatTensor)\ny = y.type(torch.FloatTensor)\n\ny = torch.where(y == 1.0, 1.0, -1.0)\n\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 100)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x,y,**kwargs)\n\nAs we can see in the following visualization, my implementation of the perceptron algorithm takes 13 update steps to find a separating hyperplane between the setosa species and the rest of the data, when considering the features Sepal Length and Petal Length.\n\ntorch.manual_seed(2)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams['figure.figsize'] = (14, 9)\nfig, axarr = plt.subplots(3, 5, sharex= True, sharey= True)\nmarkers = ['o', ',']\nmarker_map = {-1 : 0, 1 : 1}\n\ncurrent_ax = 0\nloss = 1.0\nmax_iter = 1000\n\n# for keeping track of loss values\nloss_vec = []\n\ncurr_iter = 0\n\nwhile loss &gt; 0.0 and curr_iter &lt; max_iter: #if data is not linearly separable, terminates after 1000 iterations\n    \n    ax = axarr.ravel()[current_ax]\n\n    # save old value of w for plotting\n    old_w = torch.clone(p.w)\n    \n    # update step on random data point\n    rand_point = torch.randint(X.shape[0], size = (1,))\n    x_i = X[[rand_point],:]\n    y_i = y[rand_point]\n    local_loss = opt.step(x_i, y_i)\n\n    # if change was made, plot old and new decision boundaries\n        # adds new loss to loss_vec for plotting\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = 4, x_max = 8, ax = ax, color = 'black', linestyle = 'dashed')\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(torch.clone(p.w), x_min = 4, x_max = 8, ax = ax, color = 'black')\n        ax.scatter(X[rand_point, 0], X[rand_point, 1], color = 'black', facecolors = 'none', edgecolors = 'black', marker = markers[marker_map[y[rand_point].item()]])\n        ax.set_title(f'loss = {loss:.3f}')\n        ax.set(xlim = (4, 8), ylim = (0, 8))\n        ax.grid(True)\n        current_ax += 1\n    curr_iter += 1\n    plt.tight_layout\n\n\n\n\n\n\n\n\nHere is a graph showing the evolution of the loss over the course of the perceptron algorithm. Notice that the graph terminates before the maximum number of iterations at step 13. This shows again that the perceptron converged in 13 update steps.\n\nplt.figure(figsize= (8,8))\nplt.plot(loss_vec, color = \"slategrey\")\n\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Perceptron Loss on Linearly Separable Data 2 Dimensions\")\n\n\n\n\n\n\n\n\nNow, we will do that again on data that is not linearly separable, that is, when the predictive class is either the Versicolor or Virginica species of Iris flower (we will select the Versicolor species arbitrarily here). Again, I am creating a binary label of whether a given data point is of the Versicolor species or not.\n\nspecies_versicolor = df['Species'] == 'Iris-versicolor'\ny2 = torch.from_numpy(species_versicolor.values)\n\ny2 = y2.type(torch.FloatTensor)\n\ny2 = torch.where(y2 == 1.0, 1.0, -1.0)\n\nIn this visualization, I show the decision boundary of the final iteration of my perceptron algorithm (at the maximum iteration step, 1000). It is apparent that the decision hyperplane does not cleanly separate the Versicolor species from the rest of the data.\n\ntorch.manual_seed(34)\n\n# instantiate a model and an optimizer\np2 = Perceptron() \nopt2 = PerceptronOptimizer(p2)\n\nloss = p2.loss(X, y2).item()\nmax_iter = 1000\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.shape[0]\ncurr_iter = 0\n\nultimate_w = torch.zeros_like(p2.w)\n\nwhile loss &gt; 0.0 and curr_iter &lt;= max_iter: #if data is not linearly separable, terminates after 1000 iterations\n    \n    # update step on random data point\n    rand_point = torch.randint(n, size = (1,))\n    x_i = X[[rand_point],:]\n    y2_i = y2[rand_point]\n    opt2.step(x_i, y2_i)\n\n    loss = p2.loss(X, y2).item()\n    loss_vec.append(loss)\n\n    if curr_iter == max_iter:\n        ultimate_w = torch.clone(p2.w)\n\n    curr_iter += 1\n\nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\nax.set(xlim = (4, 8), ylim = (0, 8))\nax.set_title(f'Loss = {loss:.3f}')\nax.grid(True)\nplot_perceptron_data(X, y2, ax)\ndraw_line(ultimate_w, x_min = 2, x_max = 8, ax = ax, color = \"red\")\n\n\n\n\n\n\n\n\nAs can be seen in the following visualization, the perceptron algorithm iterates until the max_iter limit of 1000, and does not terminate at a loss of 0.0. This means that the perceptron has not found a hyperplane that perfectly separates the target class from the rest of the data.\n\nplt.figure(figsize= (8,8))\nplt.plot(loss_vec, color = \"slategrey\")\n\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Perceptron Loss on Non-Linearly Separable Data with 2 Dimensions\")\n\n\n\n\n\n\n\n\nHere I am using all four of the features in the Iris data set and constructing a dummy feature with the value of 1 across all data points. In this way, I have converted the data to be fifth-dimensional, and kept its linear-separability, when classifying the binary Setosa species label. Consequently, I am reverting back to the binary Setosa label (labeling whether the data point belongs to the Setosa class or not).\n\nx1 = torch.tensor(df['SepalLengthCm'].values)\nx2 = torch.tensor(df['PetalLengthCm'].values)\nx3 = torch.tensor(df['SepalWidthCm'].values)\nx4 = torch.tensor(df['PetalWidthCm'].values)\nx5 = torch.zeros(x1.shape[0]) #arbitrary feature to create a fifth feature\nbias = torch.ones(x1.shape[0])\n\nX = torch.stack((x1, x2, x3, x4, x5, bias), -1)\n\nspecies_setosa = df['Species'] == 'Iris-setosa'\ny = torch.from_numpy(species_setosa.values)\n\nX = X.type(torch.FloatTensor)\ny = y.type(torch.FloatTensor)\n\ny = torch.where(y == 1.0, 1.0, -1.0)\n\n\n# instantiate a model and an optimizer\np3 = Perceptron() \nopt3 = PerceptronOptimizer(p3)\n\nloss = p3.loss(X, y).item()\nmax_iter = 1000\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.shape[0]\ncurr_iter = 0\n\nwhile loss &gt; 0.0 and curr_iter &lt;= max_iter: #if data is not linearly separable, terminates after 1000 iterations\n    \n    # update step on random data point\n    rand_point = torch.randint(n, size = (1,))\n    x_i = X[[rand_point],:]\n    y_i = y[rand_point]\n    opt3.step(x_i, y_i)\n\n    loss = p3.loss(X, y).item()\n    loss_vec.append(loss)\n\n    curr_iter += 1\n\nIn the following visualization that plots the evolution of the loss against the number of update steps that my perceptron implementation makes, it can be seen that the algorithm converges to a loss of 0.0 at around step 27. This shows that the perceptron algorithm has found a hyperplane that separates the data belonging to the Setosa species from the rest of the data. Therefore, the dataset that the perceptron is acting upon is linearly separable.\n\nplt.figure(figsize= (8,8))\nplt.plot(loss_vec, color = \"slategrey\")\n\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Perceptron Loss on Linearly Separable Data with 5 Dimensions\")\n\n\n\n\n\n\n\n\n\n\nPart C: Minibatch Perceptron\nIn this section, I have implemented the mini-batch perceptron algorithm, which computes an update to the perceptron’s weight vector using \\(k\\) points at once, rather than a single point at each step \\(t\\). I will be performing experiments to confirm the functionality of my implementation, as well as confirming various properties of the minibatch perceptron. Namely: * When \\(k = 1\\), the minibatch perceptron will perform similarly to the regular perceptron. * When \\(k = 10\\), the minibatch perceptron will still find a separating line in 2-dimensional data. * When \\(k = n\\), the minibatch perceptron can converge even when the data is not linearly separable, provided that the learning rate \\(\\alpha\\) is small enough.\nFirst, I re-initialized all of the feature vectors and label vectors, creating two different label vectors: y1, containing linearly separable data, and y2, containing non-linearly separable data.\n\nx1 = torch.tensor(df['SepalLengthCm'].values)\nx2 = torch.tensor(df['PetalLengthCm'].values)\nbias = torch.ones(x1.shape[0])\nX = torch.stack((x1, x2, bias), -1)\n\ny = torch.from_numpy(species_setosa.values)\ny2 = torch.from_numpy(species_versicolor.values)\n\nX = X.type(torch.FloatTensor)\ny = y.type(torch.FloatTensor)\ny2 = y2.type(torch.FloatTensor)\n\ny = torch.where(y == 1.0, 1.0, -1.0)\ny2 = torch.where(y2 == 1.0, 1.0, -1.0)\n\nI then trained a minibatch perceptron model with a \\(k\\) value of 1 and a classic perceptron model on the linearly separable data, and compared their functionality.\n\ntorch.manual_seed(1)\n\n# instantiate a model and an optimizer\nmini_p = Perceptron() \nmini_opt = PerceptronOptimizer(mini_p)\n\nclassic_p = Perceptron()\nclassic_opt = PerceptronOptimizer(classic_p)\n\nmini_loss = mini_p.loss(X, y).item()\n\nmax_iter = 1000\n\n# performing training loop for minibatch perceptron\nmini_loss_vec = []\n\ncurr_iter = 0\n\nwhile mini_loss &gt; 0.0 and curr_iter &lt;= max_iter: #if data is not linearly separable, terminates after 1000 iterations\n    \n    # update step with batch size k = 1\n    \n    mini_opt.step_minibatch(X, y, alpha= 0.001, k= 1)\n\n    mini_loss = mini_p.loss(X, y).item()\n    mini_loss_vec.append(mini_loss)\n\n    curr_iter += 1\n\n\n# performing training loop for classic perceptron, for comparison\nclassic_loss = classic_p.loss(X, y).item()\n\nclassic_loss_vec = []\n\nn = X.shape[0]\ncurr_iter = 0\n\nwhile classic_loss &gt; 0.0 and curr_iter &lt;= max_iter: #if data is not linearly separable, terminates after 1000 iterations\n    \n    # update step on random point\n    rand_point = torch.randint(n, size = (1,))\n    x_i = X[[rand_point],:]\n    y_i = y[rand_point]\n    classic_opt.step(x_i, y_i)\n\n    classic_loss = classic_p.loss(X, y).item()\n    classic_loss_vec.append(classic_loss)\n\n    curr_iter += 1\n\n\nmini_w = torch.clone(mini_p.w)\nclassic_w = torch.clone(classic_p.w)\n\nI plotted the data, and drew the decision boundaries of both models (the minibatch perceptron with \\(k=1\\) in black, and the classic perceptron in red). As can be seen, their functionalities are comparable. Both models converge to a decision boundary that separates the two classes with 0.0 loss.\n\nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\nax.set(xlim = (4, 8), ylim = (0, 8))\nax.set_title('Performance Comparison of Minibatch and Classic Perceptron')\nax.grid(True)\nplot_perceptron_data(X, y, ax)\ndraw_line(mini_w, x_min = 2, x_max = 8, ax = ax, color = \"black\")\ndraw_line(classic_w, x_min = 2, x_max = 8, ax = ax, color = \"red\")\n\n\n\n\n\n\n\n\nThen, I trained a minibatch perceptron with a \\(k\\) value of 10 on the linearly separable 2-dimensional data.\n\n# instantiate a model and an optimizer\nmini_p = Perceptron() \nmini_opt = PerceptronOptimizer(mini_p)\n\nmini_loss = mini_p.loss(X, y).item()\n\nmax_iter = 1000\n\n# performing training loop for minibatch perceptron\nmini_loss_vec = []\n\ncurr_iter = 0\n\nwhile mini_loss &gt; 0.0 and curr_iter &lt;= max_iter: #if data is not linearly separable, terminates after 1000 iterations\n    \n    # update step with batch size k = 1\n    \n    mini_opt.step_minibatch(X, y, alpha= 0.1, k= 10)\n\n    mini_loss = mini_p.loss(X, y).item()\n    mini_loss_vec.append(mini_loss)\n\n    curr_iter += 1\n\nmini_w = torch.clone(mini_p.w)\n\nI then plotted the decision boundary of this minibatch perceptron. As can be seen, the model found a separating line on 2-dimensional data.\n\nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\nax.set(xlim = (4, 8), ylim = (0, 8))\nax.set_title('Minibatch Perceptron with k = 10')\nax.grid(True)\nplot_perceptron_data(X, y, ax)\ndraw_line(mini_w, x_min = 2, x_max = 8, ax = ax, color = \"black\")\n\n\n\n\n\n\n\n\nFor the final minibatch perceptron experiment, I trained a model with a \\(k\\) value equal to \\(n\\), the number of data points, upon non-linearly separable data (y2).\n\n# instantiate a model and an optimizer\nmini_p = Perceptron() \nmini_opt = PerceptronOptimizer(mini_p)\n\nmini_loss = mini_p.loss(X, y2).item()\n\nmax_iter = 1000\n\n# performing training loop for minibatch perceptron\nmini_loss_vec = []\n\ncurr_iter = 0\n\nwhile mini_loss &gt; 0.0 and curr_iter &lt;= max_iter: #if data is not linearly separable, terminates after 1000 iterations\n    \n    # update step with batch size k = 1\n    \n    mini_opt.step_minibatch(X, y2, alpha= 0.0002, k= y2.shape[0])\n\n    mini_loss = mini_p.loss(X, y2).item()\n    mini_loss_vec.append(mini_loss)\n\n    curr_iter += 1\n\nmini_w = torch.clone(mini_p.w)\n\nI then plotted the evolution of the minibatch perceptron’s loss. Even on non-linearly separable data, the decision boundary of the model converges to a value of around 0.55.\n\nplt.figure(figsize= (8,8))\nplt.plot(mini_loss_vec, color = \"slategrey\")\n\nplt.scatter(torch.arange(len(mini_loss_vec)), mini_loss_vec, color = \"slategrey\")\n\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"Loss\", title = \"Minibatch Perceptron Loss on Non-Linearly Separable Data\")\n\n\n\n\n\n\n\n\n\n\nPart D: Writing\nQuestion: What is the runtime complexity of a single iteration of the perceptron algorithm?\nThe runtime complexity of a single iteration of the perceptron algorithm is \\(O(p)\\), where \\(p\\) is the number of features. In a single iteration, the algorithm computes the dot product of one random data point’s feature vector with the perceptron’s weight vector, and updates the weights if a misclassification occurs. If \\(p\\), the number of features were to increase, the algorithm would need to make more calculations within the dot product for the one random data point, linearly increasing the runtime.\nQuestion: What is the runtime complexity of a single iteration of the minibatch perceptron algorithm?\nSince we can say that a single iteration of the minibatch perceptron is similar to running \\(b\\) number of iterations of the perceptron algorithm, where \\(b\\) is the batch size, we can infer that the runtime complexity of a single iteration of the minibatch perceptron is \\(O(b\\times p)\\), where \\(b\\) is the batch size and \\(p\\) is the number of features. Again, a single iteration of the minibatch perceptron algorithm is analogous to \\(b\\) iterations of the classical perceptron algorithm.\n\n\nConclusion\nIn this blog post, I constructed a working implementation of the perceptron algorithm and first tested it successfully against a synthetic linearly separable dataset. I then experimented with my algorithm using the Iris dataset, which could be modified to be a linearly separable dataset, a non-linearly separable dataset (depending on the class being classified), or a dataset with five features. Through my experimentation, I found that my perceptron worked correctly against a linearly separable dataset with 2 dimensions, predictably did not terminate on a non-linearly separable dataset with 2 dimensions, and successfully found a hyperplane that separated a linearly separable dataset with 5 dimensions.\nI then returned to this blog post to implement the minibatch perceptron algorithm, confirming its functionality by experimenting with batch sizes of 1, 10, and \\(n\\), where \\(n\\) is the number of data points."
  },
  {
    "objectID": "posts/NewtonPost/index.html",
    "href": "posts/NewtonPost/index.html",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nAbstract\nIn this blog post, I utilized my past implementation of a logistic regression model, and extended it by creating an optimizer that used Newton’s Method for optimization. Newton’s Method is a second-order optimization technique, and as such, is expected to converge the logistic regression model’s loss to an acceptable answer in fewer iterations. To confirm my optimizer’s functionality, I conducted three experiments that illustrated my model converging properly when using Newton’s Method as an optimizer, Newton’s Method converging in fewer iterations than standard gradient descent, and the property of Newton’s Method failing to converge when the learning rate \\(\\alpha\\) is too large. I finished this blog post by analyzing the computational cost of Newton’s Method and standard gradient descent and then comparing the two.\n\n\nPart A: Implement NewtonOptimizer\nHere is a link to my logistic.py file, which contains my documented implementation of the logistic regression model and NewtonOptimizer which I use throughout this blog post.\n\n\nPart B: Perform Experiments\nHere is a code chunk to generate arbitrary data with two labels that my logistic regression model will be able to classify throughout my experiments.\n\nimport torch\nfrom matplotlib import pyplot as plt\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n\nExperiment 1: Choosing \\(\\alpha\\)\nHypothesis: When \\(\\alpha\\) is chosen appropriately, Newton’s Method converges to the correct choice of w.\nBelow, I generated 2-dimensional classification data with 500 data points for my logistic regression model to be trained on. I then instantiated a logistic regression model, as well as my NewtonOptimizer, which implements Newton’s Method. I then ran a training loop for 200 iterations, and used an \\(\\alpha\\) value of 200 for my optimization steps.\n\ntorch.manual_seed(123)\nX, y = classification_data(n_points= 500, noise= 0.5, p_dims= 2)\n\nLR_newt = LogisticRegression()\nopt_newt = NewtonOptimizer(LR_newt)\n\nloss_vec_newt = []\nfor _ in range(200):\n    local_loss = LR_newt.loss(X, y).item()\n    opt_newt.step(X, y, alpha= 200)\n    \n    loss_vec_newt.append(local_loss)\n\nprint(loss_vec_newt)\n\n[0.5663606524467468, 0.38903510570526123, 0.3066951036453247, 0.25806131958961487, 0.22755499184131622, 0.20831289887428284, 0.19647885859012604, 0.18954026699066162, 0.18572716414928436, 0.18378372490406036, 0.1828673779964447, 0.18246540427207947, 0.1822993904352188, 0.18223392963409424, 0.18220892548561096, 0.1821995973587036, 0.18219614028930664, 0.18219491839408875, 0.18219442665576935, 0.1821942776441574, 0.18219421803951263, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219417333602905, 0.18219418823719025, 0.18219418823719025, 0.18219417333602905, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025, 0.18219418823719025]\n\n\nIn this visualization, I have plotted the evolution of loss of my logistic regression model using Newton’s Method of optimization. As can be seen, the loss converges very quickly to a value of around 0.18. The loss converges in about 10 optimization steps.\n\nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\nax.grid(True)\nax.plot(loss_vec_newt, color = \"slategrey\")\nax.scatter(torch.arange(len(loss_vec_newt)), loss_vec_newt, s = 20, color = \"slategrey\")\nax.set(xlim= (0, 50))\nlabs = ax.set(xlabel = \"Gradient Descent Step #\", ylabel = \"Loss\", title = \"Evolution of Loss using Newton's Method\")\n\n\n\n\n\n\n\n\n\n\nExperiment 2: Speed of Convergence\nHypothesis: Under at least some circumstances, Newton’s Method can converge much faster than standard gradient descent, in the sense of decreasing the empirical risk.\nTo compare the speed of Newton’s Method against the speed of standard gradient descent, I instantiated and trained another logistic regression model that used standard gradient descent optimization.\n\nLR_grad = LogisticRegression()\nopt_grad = GradientDescentOptimizer(LR_grad)\n\nloss_vec_grad = []\nfor _ in range(200):\n    local_loss = LR_grad.loss(X, y).item()\n    opt_grad.step(X, y, alpha= 0.1, beta= 0.1)\n    \n    loss_vec_grad.append(local_loss)\n\nprint(loss_vec_grad)\n\n[0.6792325973510742, 0.6971638202667236, 0.6887470483779907, 0.6778265237808228, 0.6669315695762634, 0.6563128232955933, 0.6459912061691284, 0.6359645128250122, 0.6262282133102417, 0.6167770028114319, 0.607605516910553, 0.5987083911895752, 0.5900797843933105, 0.5817137360572815, 0.5736044645309448, 0.5657459497451782, 0.5581319332122803, 0.5507563352584839, 0.5436127781867981, 0.5366951823234558, 0.5299970507621765, 0.5235123038291931, 0.5172346830368042, 0.5111578702926636, 0.5052757859230042, 0.4995822012424469, 0.49407121539115906, 0.4887368679046631, 0.4835732579231262, 0.47857466340065, 0.47373542189598083, 0.46904993057250977, 0.4645128548145294, 0.4601188600063324, 0.45586276054382324, 0.4517396092414856, 0.4477444589138031, 0.44387251138687134, 0.4401191771030426, 0.43647998571395874, 0.4329506456851959, 0.4295268952846527, 0.426204651594162, 0.42298004031181335, 0.41984930634498596, 0.41680872440338135, 0.41385483741760254, 0.41098424792289734, 0.4081936776638031, 0.40547993779182434, 0.40284019708633423, 0.40027138590812683, 0.39777079224586487, 0.3953357934951782, 0.3929637372493744, 0.3906523585319519, 0.3883990943431854, 0.3862018287181854, 0.38405841588974, 0.3819667398929596, 0.37992486357688904, 0.3779308497905731, 0.37598296999931335, 0.37407952547073364, 0.3722188174724579, 0.3703991770744324, 0.36861926317214966, 0.3668775260448456, 0.3651726543903351, 0.3635033071041107, 0.3618682622909546, 0.36026623845100403, 0.3586961627006531, 0.3571569621562958, 0.355647474527359, 0.35416680574417114, 0.352713942527771, 0.35128796100616455, 0.34988799691200256, 0.34851330518722534, 0.3471629023551941, 0.34583619236946106, 0.3445323407649994, 0.34325066208839417, 0.34199047088623047, 0.3407512605190277, 0.3395322263240814, 0.3383328914642334, 0.3371526300907135, 0.33599090576171875, 0.33484724164009094, 0.3337211012840271, 0.332612007856369, 0.33151954412460327, 0.3304431438446045, 0.3293825387954712, 0.3283372223377228, 0.327306866645813, 0.326291024684906, 0.32528936862945557, 0.3243015706539154, 0.3233272433280945, 0.3223660886287689, 0.32141783833503723, 0.3204821050167084, 0.31955868005752563, 0.31864720582962036, 0.3177475035190582, 0.3168592154979706, 0.3159821629524231, 0.3151160776615143, 0.3142607510089874, 0.31341585516929626, 0.31258127093315125, 0.31175678968429565, 0.310942143201828, 0.31013715267181396, 0.3093416094779968, 0.3085554242134094, 0.30777832865715027, 0.30701014399528503, 0.3062507212162018, 0.3054998815059662, 0.3047574460506439, 0.3040233254432678, 0.3032973110675812, 0.30257928371429443, 0.30186909437179565, 0.3011665642261505, 0.30047157406806946, 0.2997840642929077, 0.29910382628440857, 0.2984307110309601, 0.29776471853256226, 0.2971056401729584, 0.2964533567428589, 0.29580777883529663, 0.29516878724098206, 0.2945363223552704, 0.2939102053642273, 0.2932903468608856, 0.2926766872406006, 0.29206913709640503, 0.2914675176143646, 0.29087185859680176, 0.29028192162513733, 0.2896977663040161, 0.2891192138195038, 0.28854620456695557, 0.2879786491394043, 0.2874164879322052, 0.2868596315383911, 0.2863079607486725, 0.28576144576072693, 0.28522002696990967, 0.28468358516693115, 0.2841521203517914, 0.2836254835128784, 0.2831036448478699, 0.2825864851474762, 0.2820740342140198, 0.28156620264053345, 0.2810628414154053, 0.28056401014328003, 0.2800695598125458, 0.27957940101623535, 0.2790936231613159, 0.2786119878292084, 0.27813461422920227, 0.2776613235473633, 0.2771921157836914, 0.27672693133354187, 0.2762657105922699, 0.2758083641529083, 0.27535492181777954, 0.274905264377594, 0.2744593918323517, 0.27401721477508545, 0.2735787332057953, 0.27314385771751404, 0.2727125585079193, 0.2722848057746887, 0.2718605399131775, 0.27143973112106323, 0.27102234959602356, 0.2706082761287689, 0.2701975703239441, 0.2697901129722595, 0.2693859338760376, 0.26898491382598877, 0.2685870826244354, 0.26819244027137756, 0.26780083775520325, 0.26741233468055725, 0.26702681183815, 0.26664429903030396, 0.2662647068500519, 0.2658880949020386, 0.2655142843723297, 0.26514339447021484]\n\n\nI then plotted the evolution of the loss of both models to compare them, with the loss of the model using Newton’s Method in green and the loss of the model using standard gradient descent in red. It is apparent that the loss of model using Newton’s Method converges in fewer steps than the loss of the model using standard gradient descent. While it takes Newton’s Method around 10 optimization steps to converge, it takes standard gradient descent at least 100 steps to converge (it seems that the loss for this model is beginning to converge around 100, although I have only plotted the loss to 100 iterations).\n\nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\nax.grid(True)\nax.plot(loss_vec_grad, color = \"red\")\nax.scatter(torch.arange(len(loss_vec_grad)), loss_vec_grad, s = 20, color = \"red\")\nax.plot(loss_vec_newt, color = \"green\")\nax.scatter(torch.arange(len(loss_vec_newt)), loss_vec_newt, s = 20, color = \"green\")\nlabs = ax.set(xlabel = \"Gradient Descent Step #\", ylabel = \"Loss\", title = \"Comparison of Loss Evolution: Newton's Method vs. Gradient Descent\")\n\n\n\n\n\n\n\n\n\n\nExperiment 3: Failure to Converge\nHypothesis: If \\(\\alpha\\) is too large, Newton’s Method fails to converge.\nTo test this, I instantiated and trained another logistic regression model that used Newton’s Method for optimization. I began training the model with an \\(\\alpha\\) value of 1000, and then steadily increased and decreased the value until the evolution of loss failed to converge. I eventually found that Newton’s Method fails to converge upon this data when using an \\(\\alpha\\) value of at least 1015.\n\nLR_newt = LogisticRegression()\nopt_newt = NewtonOptimizer(LR_newt)\n\nloss_vec_newt = []\nfor _ in range(100):\n    local_loss = LR_newt.loss(X, y).item()\n    opt_newt.step(X, y, alpha= 1015)\n    \n    loss_vec_newt.append(local_loss)\n\nprint(loss_vec_newt)\n\n[0.4981890916824341, 0.2040444314479828, 0.1829507201910019, 0.18338710069656372, 0.18297818303108215, 0.18341781198978424, 0.18301132321357727, 0.18345630168914795, 0.1830507516860962, 0.18350304663181305, 0.18309730291366577, 0.18355880677700043, 0.1831519454717636, 0.18362431228160858, 0.1832159459590912, 0.18370071053504944, 0.18329083919525146, 0.1837894320487976, 0.18337838351726532, 0.18389193713665009, 0.18348078429698944, 0.18401022255420685, 0.18360063433647156, 0.18414635956287384, 0.18374072015285492, 0.1843028962612152, 0.18390445411205292, 0.18448254466056824, 0.18409545719623566, 0.18468840420246124, 0.1843176633119583, 0.1849239021539688, 0.1845751255750656, 0.18519286811351776, 0.1848718672990799, 0.1854993849992752, 0.18521182239055634, 0.18584798276424408, 0.1855984926223755, 0.18624359369277954, 0.1860351860523224, 0.1866917461156845, 0.1865246295928955, 0.18719840049743652, 0.1870698183774948, 0.18776987493038177, 0.18767385184764862, 0.18841281533241272, 0.18834058940410614, 0.1891338974237442, 0.18907418847084045, 0.18993951380252838, 0.18988028168678284, 0.1908358484506607, 0.1907646209001541, 0.1918284296989441, 0.19173328578472137, 0.1929221749305725, 0.19279208779335022, 0.19412146508693695, 0.19394567608833313, 0.19542941451072693, 0.1951974332332611, 0.19684751331806183, 0.19654926657676697, 0.19837622344493866, 0.19800043106079102, 0.20001257956027985, 0.19954748451709747, 0.20175133645534515, 0.20118489861488342, 0.20358389616012573, 0.2029033899307251, 0.20549814403057098, 0.20469054579734802, 0.2074785828590393, 0.20653103291988373, 0.2095061093568802, 0.20840707421302795, 0.21156039834022522, 0.21029846370220184, 0.2136181741952896, 0.21218419075012207, 0.2156571000814438, 0.2140435427427292, 0.2176545262336731, 0.21585562825202942, 0.21958953142166138, 0.21760354936122894, 0.22144532203674316, 0.21927222609519958, 0.22320714592933655, 0.22085095942020416, 0.22486603260040283, 0.22233131527900696, 0.22641535103321075, 0.22370930016040802, 0.22785264253616333, 0.22498372197151184, 0.2291775792837143]\n\n\nI then created a visualization showing the evolution of the loss using Newton’s Method. As can be seen, the loss converged until about optimization step 40, where it then began to increase steadily, thereby diverging. I highlighted this section of the evolution in red.\n\nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\nax.grid(True)\nax.plot(loss_vec_newt, color = \"slategrey\")\nax.scatter(torch.arange(len(loss_vec_newt)-60), loss_vec_newt[:40], s = 20, color = \"slategrey\")\nax.scatter(torch.arange(40, len(loss_vec_newt)), loss_vec_newt[40:], s = 20, color = \"red\")\nlabs = ax.set(xlabel = \"Gradient Descent Step #\", ylabel = \"Loss\", title = \"Evolution of Loss using Newton's Method with Large Alpha Value\")\n\n\n\n\n\n\n\n\n\n\n\nPart C: Operation Counting\nGiven assumptions, where \\(c\\) is an arbitrary computational unit and \\(p\\times p\\) is the size of the Hessian matrix computed in Newton’s Method: * \\(O(\\text{calculate } L) = c\\) * \\(O(\\text{calculate } \\nabla L) = 2c\\) * \\(O(\\text{calculate } H) = pc\\) * \\(O(\\text{invert }M^{p\\times p}) = k_1p^\\gamma\\), where \\(2\\leq \\gamma &lt;3\\) * \\(O(\\text{multiply } v^{1\\times p}M^{p\\times p}) = k_2p^2\\)\nSupposing that Newton’s Method converges to a solution in \\(t_{nm}\\) steps and standard gradient descent converges to a solution in \\(t_{gd}\\) steps, I have written expressions below that describe the total computational cost of each optimization method.\n\\[\n\\begin{aligned}\nO(\\text{Gradient Descent}) &= O(L) + t_{gd}*O(\\nabla L) \\\\\n&= c + 2t_{gd}c \\\\\nO(\\text{Newton's Method}) &= O(L) + t_{nm}*\\left(O(\\nabla L) + O(H) + O(\\text{invert } M^{p\\times p}) + O(v^{1\\times p}M^{p\\times p})\\right) \\\\\n&= c + t_{nm}*(2c + pc + k_1p^\\gamma + k_2p^2)\n\\end{aligned}\n\\]\nI then put both of these expressions into an inequality to determine how much smaller \\(t_{nm}\\) must be than \\(t_{gd}\\) in order for Newton’s Method to require fewer computations \\(c\\) to complete. By manipulating this equality, I found that \\(t_{nm}\\) must be smaller than \\(t_{gd}\\) by a factor of \\((1 + \\frac{p}{2} + \\frac{k_1p^\\gamma}{2c}+\\frac{k_2p^2}{2c})\\), for Newton’s Method to be less computational expensive than standard gradient descent. This polynomial expression is dominated by the terms \\(k_1p^\\gamma\\) and \\(k_2p^2\\), and can be simplified as such. Therefore, when \\(p\\) becomes very large, it is increasingly unlikely that Newton’s Method will be less computationally expensive than standard gradient descent, unless it takes a logistic regression model using Newton’s Method \\(\\frac{1}{p^2}\\) the number of steps to converge relative to a model using standard logistic regression.\n\\[\n\\begin{aligned}\nO(\\text{Gradient Descent}) &&gt; O(\\text{Newton's Method}) \\\\\nc + 2t_{gd}c &&gt; c + t_{nm}*(2c + pc + k_1p^\\gamma + k_2p^2) \\\\\n2t_{gd}c &&gt; t_{nm}*(2c + pc + k_1p^\\gamma + k_2p^2) \\\\\nt_{gd} &&gt; \\frac{t_{nm}}{2c}*(2c + pc + k_1p^\\gamma + k_2p^2) \\\\\nt_{gd} &&gt; t_{nm}*(1 + \\frac{p}{2} + \\frac{k_1p^\\gamma}{2c} + \\frac{k_2p^2}{2c})\n\\end{aligned}\n\\]\n\n\nConclusion\nIn conclusion, my blog post is an extension of my previous blog post concerning logistic regression. My extension included the integration of Newton’s Method as an optimizer for my logistic regression model. Newton’s Method, being a second-order optimization technique, offers the potential for faster convergende, as evidenced by my experiments. In these experiments, I demonstrated the proper convergence of my optimizer, its efficiency in requiring fewer iterations compared to standard gradient descent, and the sensitivity of Newton’s Method to large learning rates of \\(\\alpha\\). Furthermore, an analysis of computational costs revealed valuable insights into the trade-offs between Newton’s Method and standard gradient descent; namely, how the the complexity of Newton’s Method is heavily influenced by the dimensions of the data, \\(p\\). By showcasing the advantages and limitations of both methods, this blog posts illustrates a deeper understanding of optimization techniques in logistic regression modeling."
  },
  {
    "objectID": "posts/LoanPost/index.html",
    "href": "posts/LoanPost/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "Abstract\nThis blog post presents the development and application of a machine learning-based logistic regression model to assess the risk of loan default associated with loan applicants and subsequently determining the optimal threshold for maximizing profit for a bank. The model leverages various applicant features to assign risk scores, enabling precise categorization of applicants into high and low-risk groups. Through further analysis, the optimal threshold for loan approval is identified, balancing the trade-off between minimizing risk and maximizing profitability. By utilizing this threshold, the model decides to approve or deny loans based on the risk scores of individual applicants. Upon training the model, the trained weights of the model showed that the most influential features for a potential loan applicant were the loan amount and the applicant’s income.\n\n\nPart A: Grab the Data\nHere I import the data and get a quick snapshot of the column names and types of data within it.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nPart B: Explore the Data\nCreate at least two visualizations and one summary table in which you explore patterns in the data. You might consider some questions like:\n\nHow does loan intent vary with the age, length of employment, or homeownership status of an individual?\nWhich segments of prospective borrowers are offered low interest rates? Which segments are offered high interest rates?\nWhich segments of prospective borrowers have access to large lines of credit?\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nFor my first visualization, I created a density plot of the initial loan rate of a loan applicant vs. their income, with color mapping representing whether the person had a past loan default in their history. One trend that is most apparent is the clear divide between loan applicants that have had a past default and those that haven’t. Across all income levels, there is a seemingly constant difference regarding initial loan rates between those who have had a past default and those who have not. Those who have had a past loan default on their file tend to have an initial loan rate of 10% or higher, while those who have not tend to have a 10% rate or lower. The rates for those with a past loan default seem to concentrate around 12.5% and 7.5% for those without a past default, regardless of income level.\n\nplot1 = sns.displot(data= df_train, x= 'person_income', y= 'loan_int_rate', hue= 'cb_person_default_on_file')\nplt.xscale('log')\n\n# This code is not working, it just creates a separate legend that doesn't have labels\n#plt.legend(title= 'History of Default', labels= [\"No\", \"Yes\"])\n\nplot1.set(xlim= (0, 1000000))\nplot1.set(xlabel= \"Person's Income (Log Scale)\", ylabel= \"Initial Loan Rate\", title= 'Initial Loan Rate vs. Income')\nplot1\n\nc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\seaborn\\axisgrid.py:39: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  ax.set(**kwargs)\n\n\n\n\n\n\n\n\n\nI then plotted a catplot of a loan applicants income against their home ownership types. Again, there did not seem to be a strong trend, but there was a slightly higher median of income for loan applicants renting their homes as opposed to mortgaging or owning. Perhaps this correlates to a trend in which a person needs a larger income in order to afford to rent certain types of homes (i.e. expensive apartments or condos). Like the weak correlation above, it seems that not much can be extraploated from this visualization that could prove helpful to prediciting loan default rates of a given person.\n\nplot2 = sns.catplot(data= df_train, x= 'person_home_ownership', y= 'loan_int_rate', kind= \"boxen\")\nplot2.set(xlabel= \"Home Ownership Type\", ylabel= \"Initial Loan Rate\", title= 'Income Across Home Ownership Types')\nplot2\n\nNameError: name 'sns' is not defined\n\n\nFor the third visualization, I created a catplot that mapped one’s initial loan rate against whether or not they have had a past loan default. Here, I found a strong trend of higher initial loan rates for those who have previously defaulted on a loan. This correlation is intuitive, as most banks would want to protect their interests from possible future defaults through higher loan rates, and people who have defaulted on loans in the past are often thought to be more likely to default in the future, than those who have never defaulted on a loan.\n\ntemp = df_train.copy()\ntemp['cb_person_default_on_file'] = temp['cb_person_default_on_file'].str.replace('Y', 'Past Default')\ntemp['cb_person_default_on_file'] = temp['cb_person_default_on_file'].str.replace('N', 'No Default')\n\n\nplot3 = sns.catplot(data= temp, x= 'cb_person_default_on_file', y= 'loan_int_rate', kind= 'boxen')\nplot3.set(xlabel= \"History of Loan Default\", ylabel= \"Initial Loan Rate\", title= 'Initial Loan Rate vs. Loan Default History')\nplot3\n\nNameError: name 'df_train' is not defined\n\n\nI then had the idea to separate all of the loan applicants by age group, as perhaps there were trends that existed between them. I chose the categories of &lt;30, 30-50, 50-70, 70-90, and 90+. I then compared the number of loan applications from each group, and found that the dataset is mostly comprised of loan applications from people younger than 30, at that there are little to no applications from people older than 50 in the data set.\n\nbins= [0, 30, 50, 70, 90, 150]\nlabels= [\"&lt;30\", \"30-50\", \"50-70\", \"70-90\", \"90+\"]\ndf_train['person_age_group'] = pd.cut(df_train[\"person_age\"], bins=bins, labels=labels, right=False)\n\nNameError: name 'pd' is not defined\n\n\n\nplot3 = sns.catplot(df_train, x= \"person_age_group\", y= None, kind= 'count')\nplot3.set(xlabel= 'Age Group', ylabel= 'Count', title= \"Number of Loans by Age Group\")\nplot3\n\nNameError: name 'sns' is not defined\n\n\nI then wanted to see the distribution of loan intents across age groups, so I created the following summary table. Within the &lt;30 group, the most common intent was education, the least common was home improvement, and the rest were mostly comparable. This is intuitive as most young people are focused on going to college or graduate school and take out loans to do so; they also often do not have houses to make improvements upon during this time in their lives. The 30-50 group generally had an even distribution of loan intents. The 50-70 group saw a large amount of loan applications with the intent of ‘personal’. Perhaps this is due to most of these people being retired, yet living outside of their means, and are thus relying on the money of bank loans to pay for some of their lifestyles instead of their old incomes.\n\nsum_table = df_train.groupby([\"person_age_group\", 'loan_intent']).size()\nsum_table\n\nC:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_44992\\2851417689.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sum_table = df_train.groupby([\"person_age_group\", 'loan_intent']).size()\n\n\nperson_age_group  loan_intent      \n&lt;30               DEBTCONSOLIDATION    3028\n                  EDUCATION            4001\n                  HOMEIMPROVEMENT      1881\n                  MEDICAL              3429\n                  PERSONAL             3129\n                  VENTURE              3340\n30-50             DEBTCONSOLIDATION    1114\n                  EDUCATION            1106\n                  HOMEIMPROVEMENT      1001\n                  MEDICAL              1363\n                  PERSONAL             1156\n                  VENTURE              1239\n50-70             DEBTCONSOLIDATION      35\n                  EDUCATION              18\n                  HOMEIMPROVEMENT        20\n                  MEDICAL                35\n                  PERSONAL              118\n                  VENTURE                34\n70-90             DEBTCONSOLIDATION       1\n                  EDUCATION               0\n                  HOMEIMPROVEMENT         0\n                  MEDICAL                 7\n                  PERSONAL                4\n                  VENTURE                 0\n90+               DEBTCONSOLIDATION       0\n                  EDUCATION               2\n                  HOMEIMPROVEMENT         0\n                  MEDICAL                 1\n                  PERSONAL                1\n                  VENTURE                 1\ndtype: int64\n\n\n\n\nPart C: Build a Model\nPlease use any technique to construct a score function and threshold for predicting whether a prospective borrower is likely to default on a given loan. You may use all the features in the data except loan_grade (and the target variable loan_status), and you may choose any subset of these. There are several valid ways to approach this modeling task:\n\nChoose features and estimate entries of a weight vector w by hand (this is allowed but not recommended).\n(Recommended): Choose your features, estimate new ones if needed, and fit a score-based machine learning model to the data. My suggestion is LogisticRegression. Once you have fit a logistic regression model, the weight vector w is stored as the attribute model.coef_.\n\nI suggest that you try several combinations of features, possibly including some which you create, and test out which combinations work best with cross-validation.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nHere I am removing the loan_grade and loan_status features from the dataset, saving loan_status as the target variable for later use. Additionally, I created ‘dummy’ variables for all of the qualitative features (e.g., person_home_mortgage, loan_intent, etc.), so that it can be processed by my logistic regression model.\n\n# HERE one-hot encode all of the qual variables\n\ndef prepare_data(df):\n  df = df.dropna()\n  y = df[\"loan_status\"]\n  df = df.drop([\"loan_status\", \"loan_grade\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\n\n\n# feature names\nfeatures = X_train.columns\nLR = LogisticRegression(max_iter= 10000)\nLR.fit(X_train[features], y_train)\n\nLogisticRegression(max_iter=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=10000)\n\n\nHere I show the optimal weights for all of the features that the model was trained on. It seems that the most important factors were the loan amount and the applicant’s income.\n\ncoefficients = LR.coef_\n\nweights_table = pd.DataFrame({'Feature': features, 'Weight': coefficients[0]})\nweights_table.sort_values('Weight', ascending= False)\n\n\n\n\n\n\n\n\nFeature\nWeight\n\n\n\n\n3\nloan_amnt\n1.065581e-04\n\n\n4\nloan_int_rate\n9.498579e-08\n\n\n10\nperson_home_ownership_RENT\n8.072591e-09\n\n\n18\ncb_person_default_on_file_Y\n7.272047e-09\n\n\n11\nloan_intent_DEBTCONSOLIDATION\n2.680009e-09\n\n\n5\nloan_percent_income\n2.549965e-09\n\n\n13\nloan_intent_HOMEIMPROVEMENT\n1.839150e-09\n\n\n14\nloan_intent_MEDICAL\n1.332706e-09\n\n\n21\nperson_age_group_50-70\n1.216076e-10\n\n\n8\nperson_home_ownership_OTHER\n5.824537e-11\n\n\n23\nperson_age_group_90+\n-2.296252e-12\n\n\n22\nperson_age_group_70-90\n-1.399148e-11\n\n\n20\nperson_age_group_30-50\n-8.286292e-10\n\n\n15\nloan_intent_PERSONAL\n-1.414263e-09\n\n\n19\nperson_age_group_&lt;30\n-1.656899e-09\n\n\n12\nloan_intent_EDUCATION\n-3.018828e-09\n\n\n16\nloan_intent_VENTURE\n-3.798981e-09\n\n\n9\nperson_home_ownership_OWN\n-3.939349e-09\n\n\n7\nperson_home_ownership_MORTGAGE\n-6.571695e-09\n\n\n17\ncb_person_default_on_file_N\n-9.652256e-09\n\n\n6\ncb_person_cred_hist_length\n-1.225154e-08\n\n\n2\nperson_emp_length\n-2.492166e-08\n\n\n0\nperson_age\n-6.362318e-08\n\n\n1\nperson_income\n-4.057367e-05\n\n\n\n\n\n\n\n\nLR.score(X_train[features], y_train)\n\n0.8080062862880342\n\n\n\n\nPart D: Find a Threshold\nGiven assumptions for our model: 1. If the loan is repaid in full, the profit for the bank is:\n$\\text{loan\\_amt}\\times(1+0.25\\times\\text{loan\\_int\\_rate})^{10} - \\text{loan\\_amt}$\n\nIf the borrower defaults on the loan, the “profit” is:\n\\(\\text{loan\\_amt}\\times(1+0.25\\times\\text{loan\\_int\\_rate})^{3}-(1.7\\times\\text{loan\\_amt})\\)\n\nAfter training my model, I had it predict the probabilities of each data point belonging to class 0 or class 1, i.e., the receiver of the bank loan either repaying or defaulting on their loan. I then created the ‘profit_likeli’ feature within the training set, which represents the probability of the loanee fully repaying the bank loan, thereby making the bank a profit. Instead of creating a “risk” score to quantify how likely the loanee is to default, I chose this metric, as the prospective loss due to a loan default is expressed as an impact on the profit in the given assumptions of the model.\n\nfrom sklearn.metrics import confusion_matrix\n\ntrain_preds = LR.predict_proba(X_train[features])\nX_train[\"profit_likeli\"] = train_preds[:, 0]\n\nI followed the equations above to create new columns in the dataset that define the cost or profit a bank can expect if the borrower defaults or repays their loan.\n\nX_train['profit'] = X_train[\"loan_amnt\"]*(1+0.25*(X_train['loan_int_rate']/100))**10 - X_train['loan_amnt']\nX_train['cost'] = X_train['loan_amnt']*(1+0.25*(X_train['loan_int_rate']/100))**3 - (1.7*X_train['loan_amnt'])\n\nAfter computing the profit likelihoods, the projected profit, and the projected cost of each loan applicant, I was then able to search for the optimal threshold by defining a function that would calculate the expected total profit of the bank if it made loan approval decisions on my profit likelihood feature using a given threshold. This function works by using the threshold to either approve or deny the loan application (0 if approve, 1 if deny, as this will match up with our y_train labels). This approval is calculated by testing whether the profit_likeli is greater than the given threshold. Then, if the approved loans are correct (match up with the actual y_train label), the expected profit will be added to the total; if the approved loans are incorrect (contradict the actual y_train labels), the expected cost will instead be added to the total.\n\ndef calculate_profit(threshold, df, y):\n    df['expect_default'] = np.where(df['profit_likeli'] &gt; threshold, 0, 1)\n    df['correct'] = np.where(df['expect_default'] == y, 1, 0)\n\n    approved = df[df['expect_default'] == 0]\n\n    return np.where(approved['correct'] == 1, approved['profit'], approved['cost']).sum()\n\nBelow, I graphed the expected profits of my model over 1000 different thresholds between 0 and 1. It appears that the optimal thresholds is somewhere close to 0.6.\n\nthresholds = np.linspace(0, 1, 1000)\nexpected_profits = [calculate_profit(threshold, X_train, y_train) for threshold in thresholds]\n\nplt.plot(thresholds, expected_profits)\nplt.title(\"Threshold Values vs. Profit\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Profit (in tens of millions)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nI then used the ‘optimize’ object from the scipy Python library to find the exact value of the optimal threshold, and the resulting maximimum profit.\n\nfrom scipy import optimize\n\nmaximize_func = lambda x: -calculate_profit(x, X_train, y_train)\n\nmaximization = optimize.minimize_scalar(maximize_func, bounds= (0, 1))\nprint(\"The optimum threshold is: \" + str(maximization.x))\nprint(\"The optimal profit, using the threshold, is: \" + str(round(-maximize_func(maximization.x), 2)))\n\nThe optimum threshold is: 0.6047470824523239\nThe optimal profit, using the threshold, is: 30850532.36\n\n\nSince there were 22,907 data points that I trained my model on, and the profit expected using my optimal threshold was 30,850,532.36, my estimate of the bank’s expected profit per borrower on the training set is 1346.77 dollars in profit per borrower.\n\n\nPart E: Evaluate Model From the Bank’s Perspective\nNow I will test my finalized weight vector and optimal threshold against the test set.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nI performed the same data pre-processing steps on the testing data as I did to the training data, and then calculated the expected profit of the bank on the testing data using my trained model and optimal threshold.\n\nbins= [0, 30, 50, 70, 90, 150]\nlabels= [\"&lt;30\", \"30-50\", \"50-70\", \"70-90\", \"90+\"]\ndf_test['person_age_group'] = pd.cut(df_test[\"person_age\"], bins=bins, labels=labels, right=False)\n\nX_test, y_test = prepare_data(df_test)\n\ntest_preds = LR.predict_proba(X_test[features])\nX_test[\"profit_likeli\"] = test_preds[:, 0]\n\nX_test['profit'] = X_test[\"loan_amnt\"]*(1+0.25*(X_test['loan_int_rate']/100))**10 - X_test['loan_amnt']\nX_test['cost'] = X_test['loan_amnt']*(1+0.25*(X_test['loan_int_rate']/100))**3 - (1.7*X_test['loan_amnt'])\n\nexpected = calculate_profit(maximization.x, X_test, y_test)\n\nprint(\"The expected profit per borrower on the test set is: \" + str(round(expected / X_test.shape[0], 2)))\n\nThe expected profit per borrower on the test set is: 1273.62\n\n\n\n\nPart F: Evaluate Model From the Borrower’s Perspective\nQuestion 1: Is it more difficult for people in certain age groups to access credit under my proposed system?\nFortunately, in my earlier data preprocessing, I have already given each loan applicant an age group designation, so it should be fairly simple to answer this question.\n\nX_test['person_age_group'] = pd.cut(df_test[\"person_age\"], bins=bins, labels=labels, right=False)\nX_test.groupby('person_age_group')['expect_default'].aggregate(np.mean)\n\nC:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_44992\\2666120760.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  X_test.groupby('person_age_group')['expect_default'].aggregate(np.mean)\nC:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_44992\\2666120760.py:2: FutureWarning: The provided callable &lt;function mean at 0x000001484D709790&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  X_test.groupby('person_age_group')['expect_default'].aggregate(np.mean)\n\n\nperson_age_group\n&lt;30      0.175320\n30-50    0.153194\n50-70    0.166667\n70-90    0.500000\n90+           NaN\nName: expect_default, dtype: float64\n\n\nWith this summary table, we can see the model’s interpretation of the average likelihood across age groups that a loan applicant will default on their loan payment, and therefore should be denied access to the bank’s credit. It looks like the model believes the age group of 70-90 to be the riskiest by far, and then those who are &lt;30 have a 17.53% expected chance of defaulting, followed by a 16.67% chance for those 50-70, and finally a 15.32% chance for those 30-50. However, as we have seen in the data exploration section of this blog post, the number of people in each age group in this data set is heavily skewed, with there being many more loan applicants on the younger side. Thus, it could be said that if there were more applicants in the 70-90 range, the model would assess their risk to be comparable to the other age groups, as opposed to the current outlier of 50% expected risk of default.\nQuestion 2: Is it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\n\nX_test.groupby('loan_intent_MEDICAL')['expect_default'].aggregate(np.mean)\n\nC:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_44992\\29578798.py:1: FutureWarning: The provided callable &lt;function mean at 0x000001484D709790&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  X_test.groupby('loan_intent_MEDICAL')['expect_default'].aggregate(np.mean)\n\n\nloan_intent_MEDICAL\nFalse    0.160155\nTrue     0.209692\nName: expect_default, dtype: float64\n\n\nIt appears that my model on average assigns a loan default risk score of 5% higher to those applying for a loan to pay off medical expenses than applicants intending to use the loan for other purposes.\n\ndf_test.groupby('loan_intent')['loan_status'].aggregate(np.mean)\n\nC:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_44992\\3309084637.py:1: FutureWarning: The provided callable &lt;function mean at 0x000001484D709790&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  df_test.groupby('loan_intent')['loan_status'].aggregate(np.mean)\n\n\nloan_intent\nDEBTCONSOLIDATION    0.279497\nEDUCATION            0.167421\nHOMEIMPROVEMENT      0.246088\nMEDICAL              0.281553\nPERSONAL             0.219227\nVENTURE              0.145701\nName: loan_status, dtype: float64\n\n\nWhile my model’s expected default rate for medical expense loans is lower than the true default rate for medical expense loans (21% vs. 28%), it does reflect the fact that medical expense loans are more likely to be defaulted on than any other loan purposes.\n\nX_test.groupby('loan_intent_EDUCATION')['expect_default'].aggregate(np.mean)\n\nC:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_44992\\4235614756.py:1: FutureWarning: The provided callable &lt;function mean at 0x000001484D709790&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  X_test.groupby('loan_intent_EDUCATION')['expect_default'].aggregate(np.mean)\n\n\nloan_intent_EDUCATION\nFalse    0.167728\nTrue     0.176020\nName: expect_default, dtype: float64\n\n\n\nX_test.groupby('loan_intent_VENTURE')['expect_default'].aggregate(np.mean)\n\nC:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_44992\\4016549862.py:1: FutureWarning: The provided callable &lt;function mean at 0x000001484D709790&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  X_test.groupby('loan_intent_VENTURE')['expect_default'].aggregate(np.mean)\n\n\nloan_intent_VENTURE\nFalse    0.173904\nTrue     0.147303\nName: expect_default, dtype: float64\n\n\nFurthermore, my model’s assessment of the risk score for loan applications for the purposes of education or business ventures is far lower than those for medical expenses (17.6% and 14.7% respectively, against 21%). However, the model’s risk score more accurately represents the true default rate of these two demographics, while the model’s score for medical expense loans is, counterintuitively, lower than the true loan default rate. Perhaps this reflects the fact that my model uses the features of loan amount and applicant’s income to a greater weight than loan intent when making an approval prediction.\nQuestion 3: How does a person’s income level impact the ease with which they can access credit under my decision system?\n\nplot4 = sns.catplot(data= X_test, x= 'expect_default', y= 'person_income', kind= 'boxen')\nplot4.set(yscale= 'log')\nplot4.set(xlabel= \"Expected Default (Model)\", ylabel= \"Income (Log Scale)\", title= 'Predicted Default vs. Income')\nplot4\n\nc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\seaborn\\categorical.py:1794: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\seaborn\\categorical.py:1794: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nUsing this graph, it appears that there is a large influence that a loan applicant’s income has on the model’s assessment of whether the loan is expected to default or not. The higher a person’s income is, the more likely that the model will not predict a default on loan payment. Since the scale used for income is a logarithmic scale, this advantage is likely starker than it initially appears in this visualization.\n\n\nDiscussion\nIn summary, this blog post has detailed the development and application of a machine learning-based logistic regression model for loan risk assessment and decision-making in the banking sector. Through the analysis of various applicant features, the model assigns risk scores to quantify applicants based on their expected risk of defaulting on a loan. Importantly, the post highlights the utility of threshold optimization for loan approval, showing how a bank could calculate the optimal balance between risk mitigation and profit maximization. My model gave the most importance to the data features of loan amount and applicant’s income when assessing whether the loan applicant will default on payments.\nI define fairness as the equality of opportunity between individuals with similar features, not considering features that exist outside of their sphere of control. For example, if a bank is considering two identical applicants, but one has defaulted on a loan in the past, and the other has not, I believe that it would be considered ‘fair’ for a bank to deny the loan for the applicant with a history of defaulting, as the past loan default would be a feature of an applicant that they have control over.\nHowever, I believe that this conception of ‘fairness’ is put at odds with the very notion of machine learning, in that this model’s main goal is to use historical data from many distinct loan applicants to predict the likelihood of repayment from a new applicant. In essence, this process relies heavily on features and data that lay outside the sphere of control of the new applicant, and uses this information in the decision process of loan approval. Simply because many applicants who seek loans for medical expenses have high rates of default, it will be far more difficult for new applicants seeking a loan for medical expenses to access credit. This does not adhere to my conception of fairness, as the new applicant has no control over whether the past loan applicants defaulted on or repaid their loans. Unfortunately, the use of historical data in prediction is one of the best tools available for banks to determine loan repayment success in new applicants and maximize profits."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Oscar",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Sparse Kernel Machines\n\n\n\n\n\nIn this blog post, I will implement sparse kernelized logistic regression.\n\n\n\n\n\nApr 23, 2024\n\n\nOscar Fleet\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method for Logistic Regression\n\n\n\n\n\nIn this blog post, I will implement Newton’s method for optimization and experiment upon its effects on logistic regression.\n\n\n\n\n\nApr 15, 2024\n\n\nOscar Fleet\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nIn this post, I will implement gradient descent for logistic regression and experiment with my implementations.\n\n\n\n\n\nApr 10, 2024\n\n\nOscar Fleet\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nIn this post, I will be completing an implementation of the perceptron algorithm and testing it in several experiments.\n\n\n\n\n\nApr 9, 2024\n\n\nOscar Fleet\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs?\n\n\n\n\n\nMy Blog Post for Homework 2\n\n\n\n\n\nApr 1, 2024\n\n\nOscar Fleet\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer’s Penguins\n\n\n\n\n\nMy Blog Post for Homework 1: Classifying Palmer’s Penguins\n\n\n\n\n\nFeb 21, 2024\n\n\nOscar Fleet\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/LogisticPost/index.html",
    "href": "posts/LogisticPost/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer#, NewtonOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nAbstract\nIn this blog post, I created an implementation of the logistic regression classification model, and performed some experiments using it to confirm its functioning. First, I used it to perform ‘vanilla’ gradient descent, wherein I set its learning rate \\(\\beta = 0\\). I then performed gradient descent with momentum (when the value of \\(\\beta\\) is \\(&gt; 0\\)), and observed a speed-up in loss minimization within my model. Finally, I trained my model on classification data with a greater number of features than data points, and observed the issue of overfitting, wherein the model’s accuracy is extremely high with the training data, but far more inaccurate when tested against new data.\n\n\nPart A: Implement Logistic Regression\nHere is a link to my logistic.py file, which contains my documented implementation of the logistic regression model that I use throughout this blog post.\n\n\nPart B: Experiments\nHere is a code chunk to generate arbitrary data with two labels for my logistic regression model to classify throughout my experiments:\n\nimport torch\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n\nExperiment 1: Vanilla Gradient Descent\nHypothesis: When \\(p_{dim} = 2\\), \\(\\alpha\\) is sufficiently small, and \\(\\beta = 0\\), gradient descent for logistic regression converges to a weight vector w that looks visually correct, furthermore, the loss decreases monotonically.\nBelow I generate data with 2 dimensions using the classification_data function I created earlier. I then instantiate a logistic regression model and run a 100-iteration training loop, with \\(\\alpha = 0.1\\) and \\(\\beta = 0\\). I printed the evolution of loss with each iteration, and it appears to decrease monotonically, but I will confirm this later.\n\ntorch.manual_seed(123)\nX, y = classification_data(n_points= 500, noise= 0.5, p_dims= 2)\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\nfor _ in range(1500):\n    local_loss = LR.loss(X, y).item()\n    opt.step(X, y, alpha= 0.1, beta= 0.0)\n    \n    loss_vec.append(local_loss)\n\nprint(loss_vec)\n\n[0.5663606524467468, 0.5599420070648193, 0.5537047982215881, 0.5476439595222473, 0.5417548418045044, 0.5360325574874878, 0.5304723978042603, 0.5250697135925293, 0.5198197960853577, 0.5147179961204529, 0.5097599625587463, 0.5049411654472351, 0.5002572536468506, 0.49570396542549133, 0.4912770986557007, 0.4869726002216339, 0.4827864468097687, 0.4787147343158722, 0.4747536778450012, 0.4708996117115021, 0.4671488404273987, 0.46349790692329407, 0.45994341373443604, 0.4564821124076843, 0.4531107544898987, 0.4498262405395508, 0.44662556052207947, 0.44350579380989075, 0.44046422839164734, 0.4374981224536896, 0.43460479378700256, 0.4317817687988281, 0.42902660369873047, 0.42633694410324097, 0.42371055483818054, 0.4211452007293701, 0.41863882541656494, 0.41618937253952026, 0.41379493474960327, 0.41145360469818115, 0.4091636538505554, 0.4069232642650604, 0.40473082661628723, 0.40258467197418213, 0.4004833400249481, 0.39842531085014343, 0.39640918374061584, 0.3944335877895355, 0.3924972116947174, 0.3905988037586212, 0.38873714208602905, 0.3869110643863678, 0.3851194977760315, 0.3833613693714142, 0.38163551688194275, 0.3799411654472351, 0.37827715277671814, 0.37664276361465454, 0.37503692507743835, 0.37345895171165466, 0.37190794944763184, 0.37038323283195496, 0.3688839077949524, 0.367409348487854, 0.36595889925956726, 0.36453181505203247, 0.3631274998188019, 0.36174529790878296, 0.36038464307785034, 0.35904499888420105, 0.35772576928138733, 0.3564263880252838, 0.3551464378833771, 0.35388538241386414, 0.3526427149772644, 0.35141801834106445, 0.35021084547042847, 0.34902074933052063, 0.3478473722934723, 0.34669026732444763, 0.3455491065979004, 0.34442344307899475, 0.34331297874450684, 0.342217355966568, 0.34113621711730957, 0.3400692641735077, 0.3390161991119385, 0.33797669410705566, 0.3369504511356354, 0.3359372019767761, 0.3349367082118988, 0.33394861221313477, 0.3329727351665497, 0.33200880885124207, 0.3310566246509552, 0.3301158547401428, 0.329186350107193, 0.3282678723335266, 0.32736024260520935, 0.32646316289901733, 0.3255764842033386, 0.32470008730888367, 0.3238336145877838, 0.3229769766330719, 0.322130024433136, 0.32129254937171936, 0.32046428322792053, 0.3196452558040619, 0.31883519887924194, 0.31803393363952637, 0.3172413408756256, 0.31645721197128296, 0.315681517124176, 0.3149139881134033, 0.31415459513664246, 0.3134031295776367, 0.31265950202941895, 0.3119235932826996, 0.3111951947212219, 0.3104742765426636, 0.3097606897354126, 0.30905425548553467, 0.3083549737930298, 0.3076626658439636, 0.30697718262672424, 0.30629852414131165, 0.3056265115737915, 0.30496105551719666, 0.30430206656455994, 0.3036494851112366, 0.30300310254096985, 0.30236294865608215, 0.30172884464263916, 0.3011007308959961, 0.30047857761383057, 0.29986223578453064, 0.29925161600112915, 0.2986466586589813, 0.2980473041534424, 0.2974534332752228, 0.29686498641967773, 0.2962818741798401, 0.29570403695106506, 0.2951314151287079, 0.2945638597011566, 0.29400143027305603, 0.2934439778327942, 0.2928914427757263, 0.29234373569488525, 0.29180091619491577, 0.2912627160549164, 0.29072922468185425, 0.2902003526687622, 0.2896760106086731, 0.28915613889694214, 0.28864073753356934, 0.28812965750694275, 0.28762292861938477, 0.28712043166160583, 0.28662216663360596, 0.28612804412841797, 0.2856380343437195, 0.28515204787254333, 0.2846701145172119, 0.2841920852661133, 0.28371796011924744, 0.2832476794719696, 0.2827812135219574, 0.2823185324668884, 0.28185951709747314, 0.28140419721603394, 0.280952513217926, 0.28050440549850464, 0.2800598442554474, 0.2796187698841095, 0.27918118238449097, 0.27874696254730225, 0.2783161699771881, 0.2778887152671814, 0.27746450901031494, 0.2770436108112335, 0.2766258716583252, 0.27621138095855713, 0.27580004930496216, 0.2753917872905731, 0.2749866247177124, 0.2745845317840576, 0.2741854190826416, 0.27378931641578674, 0.2733961045742035, 0.273005872964859, 0.27261847257614136, 0.27223390340805054, 0.27185219526290894, 0.2714732587337494, 0.2710970938205719, 0.2707236409187317, 0.27035290002822876, 0.26998481154441833, 0.269619345664978, 0.2692565321922302, 0.2688962519168854, 0.26853856444358826, 0.2681834101676941, 0.2678307592868805, 0.2674805521965027, 0.26713281869888306, 0.26678749918937683, 0.2664446234703064, 0.2661040723323822, 0.265765905380249, 0.2654300630092621, 0.2650964856147766, 0.264765202999115, 0.2644362151622772, 0.26410943269729614, 0.26378485560417175, 0.26346248388290405, 0.26314228773117065, 0.2628242075443268, 0.2625083029270172, 0.2621944546699524, 0.2618826925754547, 0.2615729868412018, 0.261265367269516, 0.2609597444534302, 0.26065611839294434, 0.26035448908805847, 0.2600547969341278, 0.25975707173347473, 0.25946131348609924, 0.2591674327850342, 0.25887542963027954, 0.2585853040218353, 0.25829699635505676, 0.258010596036911, 0.25772595405578613, 0.2574431896209717, 0.2571621537208557, 0.2568828761577606, 0.2566053569316864, 0.25632959604263306, 0.2560555338859558, 0.25578323006629944, 0.2555125653743744, 0.25524359941482544, 0.2549762725830078, 0.2547105848789215, 0.25444653630256653, 0.25418412685394287, 0.2539232671260834, 0.2536640167236328, 0.2534063160419464, 0.25315019488334656, 0.25289562344551086, 0.25264260172843933, 0.2523910403251648, 0.25214099884033203, 0.25189244747161865, 0.25164538621902466, 0.25139978528022766, 0.2511556148529053, 0.2509129047393799, 0.2506715953350067, 0.25043171644210815, 0.2501932382583618, 0.2499561607837677, 0.24972042441368103, 0.24948608875274658, 0.2492530643939972, 0.24902142584323883, 0.24879108369350433, 0.2485620677471161, 0.2483343780040741, 0.24810796976089478, 0.24788284301757812, 0.24765902757644653, 0.24743644893169403, 0.24721510708332062, 0.24699504673480988, 0.24677617847919464, 0.2465585619211197, 0.24634216725826263, 0.2461269497871399, 0.24591293931007385, 0.24570010602474213, 0.24548843502998352, 0.24527797102928162, 0.24506863951683044, 0.24486044049263, 0.24465340375900269, 0.2444474697113037, 0.24424265325069427, 0.24403896927833557, 0.24383635818958282, 0.243634894490242, 0.24343442916870117, 0.24323511123657227, 0.24303680658340454, 0.24283958971500397, 0.24264343082904816, 0.24244830012321472, 0.24225419759750366, 0.24206113815307617, 0.24186907708644867, 0.24167804419994354, 0.2414880096912384, 0.24129894375801086, 0.2411108762025833, 0.24092379212379456, 0.2407377064228058, 0.24055254459381104, 0.24036836624145508, 0.24018512666225433, 0.2400028258562088, 0.23982146382331848, 0.239641010761261, 0.2394615113735199, 0.23928293585777283, 0.239105224609375, 0.2389284372329712, 0.23875252902507782, 0.23857752978801727, 0.23840340971946716, 0.2382301688194275, 0.23805776238441467, 0.2378862351179123, 0.23771557211875916, 0.23754575848579407, 0.23737677931785583, 0.23720864951610565, 0.23704132437705994, 0.23687484860420227, 0.23670917749404907, 0.23654432594776154, 0.23638027906417847, 0.23621703684329987, 0.23605459928512573, 0.23589295148849487, 0.2357320487499237, 0.2355719804763794, 0.23541265726089478, 0.23525410890579224, 0.23509632050991058, 0.23493927717208862, 0.23478299379348755, 0.23462747037410736, 0.23447266221046448, 0.2343185991048813, 0.2341652810573578, 0.2340126782655716, 0.2338607758283615, 0.23370961844921112, 0.23355916142463684, 0.23340941965579987, 0.233260378241539, 0.23311203718185425, 0.23296436667442322, 0.2328173965215683, 0.2326710969209671, 0.23252549767494202, 0.23238055408000946, 0.23223629593849182, 0.23209267854690552, 0.23194974660873413, 0.23180745542049408, 0.23166580498218536, 0.23152482509613037, 0.2313844859600067, 0.231244757771492, 0.23110568523406982, 0.23096723854541779, 0.2308294028043747, 0.23069220781326294, 0.23055563867092133, 0.23041966557502747, 0.23028430342674255, 0.2301495224237442, 0.23001538217067719, 0.22988183796405792, 0.22974887490272522, 0.22961649298667908, 0.2294846922159195, 0.22935348749160767, 0.2292228490114212, 0.2290928214788437, 0.22896331548690796, 0.2288343906402588, 0.22870606184005737, 0.22857826948165894, 0.22845101356506348, 0.22832433879375458, 0.22819821536540985, 0.2280726283788681, 0.22794759273529053, 0.22782307863235474, 0.22769910097122192, 0.22757567465305328, 0.22745275497436523, 0.22733038663864136, 0.22720853984355927, 0.22708716988563538, 0.22696636617183685, 0.22684603929519653, 0.22672626376152039, 0.22660696506500244, 0.2264881581068039, 0.22636990249156952, 0.22625207901000977, 0.226134791970253, 0.2260179966688156, 0.22590167820453644, 0.22578582167625427, 0.22567050158977509, 0.22555559873580933, 0.22544123232364655, 0.2253272831439972, 0.22521387040615082, 0.22510088980197906, 0.22498837113380432, 0.2248763144016266, 0.22476470470428467, 0.22465360164642334, 0.22454290091991425, 0.22443266212940216, 0.22432288527488708, 0.22421355545520782, 0.22410468757152557, 0.22399623692035675, 0.22388821840286255, 0.22378064692020416, 0.2236735075712204, 0.22356680035591125, 0.22346052527427673, 0.22335468232631683, 0.22324924170970917, 0.22314423322677612, 0.2230396419763565, 0.22293545305728912, 0.22283169627189636, 0.22272835671901703, 0.22262543439865112, 0.22252288460731506, 0.22242076694965363, 0.22231903672218323, 0.22221767902374268, 0.22211676836013794, 0.22201623022556305, 0.2219160944223404, 0.22181634604930878, 0.2217169851064682, 0.22161801159381866, 0.22151942551136017, 0.22142119705677032, 0.22132337093353271, 0.22122591733932495, 0.22112886607646942, 0.22103212773799896, 0.22093582153320312, 0.22083987295627594, 0.2207442969083786, 0.22064904868602753, 0.2205542027950287, 0.2204596847295761, 0.22036553919315338, 0.2202717810869217, 0.22017835080623627, 0.2200852632522583, 0.2199925184249878, 0.21990016102790833, 0.21980813145637512, 0.21971645951271057, 0.21962511539459229, 0.21953414380550385, 0.21944348514080048, 0.21935315430164337, 0.21926318109035492, 0.21917355060577393, 0.219084233045578, 0.21899525821208954, 0.21890661120414734, 0.2188182920217514, 0.21873030066490173, 0.21864262223243713, 0.2185552716255188, 0.21846826374530792, 0.21838155388832092, 0.2182951718568802, 0.21820910274982452, 0.21812334656715393, 0.2180379033088684, 0.21795275807380676, 0.21786794066429138, 0.21778342127799988, 0.21769921481609344, 0.2176152914762497, 0.2175316959619522, 0.2174483984708786, 0.21736538410186768, 0.21728268265724182, 0.21720029413700104, 0.21711815893650055, 0.21703635156154633, 0.2169548124074936, 0.21687360107898712, 0.21679262816905975, 0.21671196818351746, 0.21663162112236023, 0.2165515273809433, 0.21647174656391144, 0.21639221906661987, 0.2163129597902298, 0.2162340134382248, 0.21615532040596008, 0.21607691049575806, 0.2159987837076187, 0.21592092514038086, 0.2158433198928833, 0.2157660275697708, 0.21568895876407623, 0.21561218798160553, 0.21553567051887512, 0.2154594212770462, 0.21538345515727997, 0.21530774235725403, 0.2152322679758072, 0.21515707671642303, 0.21508212387561798, 0.21500743925571442, 0.21493300795555115, 0.21485885977745056, 0.21478495001792908, 0.2147112935781479, 0.2146378606557846, 0.214564710855484, 0.21449178457260132, 0.21441912651062012, 0.21434670686721802, 0.2142745405435562, 0.2142026126384735, 0.2141309231519699, 0.2140594720840454, 0.2139882743358612, 0.2139173150062561, 0.2138465940952301, 0.2137761116027832, 0.21370583772659302, 0.21363583207130432, 0.21356603503227234, 0.21349650621414185, 0.21342718601226807, 0.21335811913013458, 0.2132892608642578, 0.21322061121463776, 0.213152214884758, 0.21308404207229614, 0.2130160927772522, 0.21294836699962616, 0.21288086473941803, 0.2128135710954666, 0.2127465158700943, 0.2126796841621399, 0.212613046169281, 0.21254666149616241, 0.21248047053813934, 0.21241448819637299, 0.21234875917434692, 0.2122832089662552, 0.21221786737442017, 0.21215274930000305, 0.21208785474300385, 0.21202315390110016, 0.211958646774292, 0.21189437806606293, 0.21183030307292938, 0.21176645159721375, 0.21170279383659363, 0.21163932979106903, 0.21157607436180115, 0.21151302754878998, 0.21145015954971313, 0.2113875150680542, 0.21132507920265198, 0.21126282215118408, 0.2112007588148117, 0.21113891899585724, 0.2110772579908371, 0.21101577579975128, 0.21095451712608337, 0.2108934372663498, 0.21083255112171173, 0.210771843791008, 0.21071135997772217, 0.21065105497837067, 0.2105909287929535, 0.21053101122379303, 0.2104712575674057, 0.2104116827249527, 0.2103523313999176, 0.21029311418533325, 0.210234135389328, 0.2101753056049347, 0.2101166695356369, 0.21005819737911224, 0.2099999338388443, 0.2099418342113495, 0.2098839432001114, 0.20982617139816284, 0.2097686380147934, 0.2097112387418747, 0.20965401828289032, 0.20959700644016266, 0.20954012870788574, 0.20948345959186554, 0.20942693948745728, 0.20937059819698334, 0.20931445062160492, 0.20925845205783844, 0.2092026323080063, 0.20914697647094727, 0.20909148454666138, 0.20903615653514862, 0.20898102223873138, 0.20892605185508728, 0.20887123048305511, 0.20881658792495728, 0.20876207947731018, 0.2087077647447586, 0.20865359902381897, 0.20859961211681366, 0.2085457742214203, 0.20849210023880005, 0.20843859016895294, 0.20838524401187897, 0.20833203196525574, 0.20827899873256683, 0.20822612941265106, 0.20817339420318604, 0.20812082290649414, 0.20806843042373657, 0.20801617205142975, 0.20796406269073486, 0.20791210234165192, 0.2078603208065033, 0.2078086882829666, 0.20775720477104187, 0.20770587027072906, 0.2076546847820282, 0.20760363340377808, 0.20755276083946228, 0.20750200748443604, 0.20745141804218292, 0.20740097761154175, 0.2073506861925125, 0.20730054378509521, 0.20725053548812866, 0.20720067620277405, 0.20715095102787018, 0.20710140466690063, 0.20705197751522064, 0.2070026844739914, 0.20695355534553528, 0.2069045454263687, 0.2068556845188141, 0.2068069577217102, 0.20675839483737946, 0.20670995116233826, 0.2066616415977478, 0.20661349594593048, 0.20656545460224152, 0.20651757717132568, 0.2064698338508606, 0.20642220973968506, 0.20637471973896027, 0.2063273787498474, 0.2062801867723465, 0.20623311400413513, 0.20618616044521332, 0.20613934099674225, 0.2060926854610443, 0.20604614913463593, 0.2059997320175171, 0.205953449010849, 0.20590728521347046, 0.20586127042770386, 0.2058153748512268, 0.2057696133852005, 0.20572397112846375, 0.20567846298217773, 0.20563308894634247, 0.20558783411979675, 0.2055426985025406, 0.20549769699573517, 0.2054528146982193, 0.20540808141231537, 0.2053634375333786, 0.20531894266605377, 0.2052745372056961, 0.20523028075695038, 0.2051861584186554, 0.20514215528964996, 0.2050982564687729, 0.20505447685718536, 0.20501083135604858, 0.20496729016304016, 0.20492388308048248, 0.20488059520721436, 0.20483741164207458, 0.20479434728622437, 0.2047514170408249, 0.20470859110355377, 0.2046658992767334, 0.2046232968568802, 0.20458084344863892, 0.2045384645462036, 0.20449621975421906, 0.20445409417152405, 0.2044120877981186, 0.20437021553516388, 0.20432843267917633, 0.20428673923015594, 0.2042451798915863, 0.2042037397623062, 0.20416240394115448, 0.2041211873292923, 0.20408004522323608, 0.204039067029953, 0.2039981633424759, 0.20395737886428833, 0.20391669869422913, 0.20387612283229828, 0.20383568108081818, 0.20379532873630524, 0.20375509560108185, 0.20371495187282562, 0.20367492735385895, 0.20363499224185944, 0.20359519124031067, 0.20355547964572906, 0.20351587235927582, 0.20347638428211212, 0.2034369856119156, 0.2033976912498474, 0.2033584862947464, 0.20331943035125732, 0.20328041911125183, 0.20324154198169708, 0.2032027542591095, 0.20316408574581146, 0.20312552154064178, 0.20308703184127808, 0.20304866135120392, 0.20301039516925812, 0.20297223329544067, 0.2029341459274292, 0.20289616286754608, 0.2028583139181137, 0.2028205245733261, 0.20278283953666687, 0.202745258808136, 0.20270776748657227, 0.2026703953742981, 0.2026330828666687, 0.20259590446949005, 0.20255880057811737, 0.20252180099487305, 0.20248490571975708, 0.20244808495044708, 0.20241138339042664, 0.20237475633621216, 0.20233823359012604, 0.2023017853498459, 0.20226545631885529, 0.20222920179367065, 0.20219305157661438, 0.20215697586536407, 0.20212103426456451, 0.20208512246608734, 0.2020493447780609, 0.20201364159584045, 0.20197802782058716, 0.20194251835346222, 0.20190706849098206, 0.20187173783779144, 0.201836496591568, 0.2018013596534729, 0.2017662674188614, 0.20173132419586182, 0.20169641077518463, 0.201661616563797, 0.20162689685821533, 0.20159225165843964, 0.20155774056911469, 0.20152325928211212, 0.2014888972043991, 0.20145462453365326, 0.20142042636871338, 0.20138628780841827, 0.20135226845741272, 0.20131832361221313, 0.2012844681739807, 0.20125070214271545, 0.20121701061725616, 0.20118340849876404, 0.20114988088607788, 0.2011164426803589, 0.20108309388160706, 0.2010498046875, 0.2010166347026825, 0.20098352432250977, 0.2009505033493042, 0.20091752707958221, 0.20088467001914978, 0.2008519023656845, 0.20081917941570282, 0.2007865607738495, 0.20075403153896332, 0.20072156190872192, 0.2006891816854477, 0.20065686106681824, 0.20062464475631714, 0.200592502951622, 0.20056042075157166, 0.20052844285964966, 0.20049653947353363, 0.20046469569206238, 0.2004329413175583, 0.20040126144886017, 0.20036965608596802, 0.20033814013004303, 0.20030666887760162, 0.20027531683444977, 0.2002440243959427, 0.20021280646324158, 0.20018166303634644, 0.20015059411525726, 0.20011958479881287, 0.20008866488933563, 0.20005781948566437, 0.20002703368663788, 0.19999633729457855, 0.1999657303094864, 0.199935182929039, 0.1999046951532364, 0.19987429678440094, 0.19984395802021027, 0.19981372356414795, 0.19978350400924683, 0.19975340366363525, 0.19972336292266846, 0.19969339668750763, 0.19966349005699158, 0.1996336579322815, 0.1996038854122162, 0.19957420229911804, 0.19954457879066467, 0.19951504468917847, 0.19948555529117584, 0.1994561403989792, 0.1994268149137497, 0.19939754903316498, 0.19936834275722504, 0.19933919608592987, 0.19931015372276306, 0.19928114116191864, 0.19925223290920258, 0.1992233544588089, 0.19919458031654358, 0.19916585087776184, 0.19913719594478607, 0.19910861551761627, 0.19908007979393005, 0.199051633477211, 0.19902323186397552, 0.19899490475654602, 0.1989666372537613, 0.19893847405910492, 0.19891034066677094, 0.19888228178024292, 0.19885428249835968, 0.1988263577222824, 0.19879849255084991, 0.1987706869840622, 0.19874295592308044, 0.19871526956558228, 0.19868765771389008, 0.19866012036800385, 0.1986326426267624, 0.19860520958900452, 0.1985778659582138, 0.19855058193206787, 0.19852334260940552, 0.19849619269371033, 0.19846905767917633, 0.1984420120716095, 0.19841505587100983, 0.19838812947273254, 0.19836126267910004, 0.1983344852924347, 0.19830773770809174, 0.19828106462955475, 0.19825445115566254, 0.1982278972864151, 0.19820141792297363, 0.19817498326301575, 0.19814860820770264, 0.1981222927570343, 0.19809603691101074, 0.19806985557079315, 0.19804371893405914, 0.1980176419019699, 0.19799163937568665, 0.19796566665172577, 0.19793976843357086, 0.19791394472122192, 0.19788813591003418, 0.1978624314069748, 0.1978367567062378, 0.19781115651130676, 0.1977856159210205, 0.19776012003421783, 0.19773468375205994, 0.19770929217338562, 0.19768397510051727, 0.1976586878299713, 0.1976335048675537, 0.1976083368062973, 0.19758325815200806, 0.19755819439888, 0.19753322005271912, 0.197508305311203, 0.19748343527317047, 0.19745862483978271, 0.19743384420871735, 0.19740913808345795, 0.19738447666168213, 0.1973598748445511, 0.19733533263206482, 0.19731083512306213, 0.1972864270210266, 0.1972620189189911, 0.19723770022392273, 0.19721341133117676, 0.19718919694423676, 0.19716501235961914, 0.1971409022808075, 0.19711683690547943, 0.19709283113479614, 0.19706888496875763, 0.1970449835062027, 0.19702111184597015, 0.19699732959270477, 0.19697357714176178, 0.19694988429546356, 0.19692622125148773, 0.19690264761447906, 0.19687910377979279, 0.19685561954975128, 0.19683218002319336, 0.19680878520011902, 0.19678546488285065, 0.19676217436790466, 0.19673894345760345, 0.19671575725078583, 0.19669261574745178, 0.19666951894760132, 0.19664648175239563, 0.19662350416183472, 0.19660058617591858, 0.19657769799232483, 0.19655485451221466, 0.19653207063674927, 0.19650933146476746, 0.19648663699626923, 0.19646398723125458, 0.1964414119720459, 0.1964188516139984, 0.1963963657617569, 0.19637390971183777, 0.19635151326656342, 0.19632916152477264, 0.19630688428878784, 0.19628460705280304, 0.1962624192237854, 0.19624023139476776, 0.19621813297271729, 0.196196049451828, 0.1961740404367447, 0.19615207612514496, 0.1961301565170288, 0.19610828161239624, 0.19608645141124725, 0.19606466591358185, 0.19604291021823883, 0.19602122902870178, 0.19599957764148712, 0.19597797095775604, 0.19595642387866974, 0.19593490660190582, 0.19591344892978668, 0.19589200615882874, 0.19587063789367676, 0.19584929943084717, 0.19582803547382355, 0.19580678641796112, 0.19578559696674347, 0.1957644522190094, 0.1957433521747589, 0.195722296833992, 0.1957012563943863, 0.19568027555942535, 0.195659339427948, 0.19563846290111542, 0.19561761617660522, 0.1955968141555786, 0.19557607173919678, 0.19555535912513733, 0.19553469121456146, 0.19551406800746918, 0.19549347460269928, 0.19547292590141296, 0.19545242190361023, 0.19543199241161346, 0.1954115629196167, 0.1953912228345871, 0.1953708827495575, 0.19535058736801147, 0.19533035159111023, 0.19531013071537018, 0.1952899843454361, 0.1952698528766632, 0.1952497810125351, 0.19522975385189056, 0.19520974159240723, 0.19518978893756866, 0.19516988098621368, 0.1951500028371811, 0.19513016939163208, 0.19511038064956665, 0.195090651512146, 0.19507092237472534, 0.19505126774311066, 0.19503162801265717, 0.19501201808452606, 0.19499249756336212, 0.19497299194335938, 0.19495351612567902, 0.19493408501148224, 0.19491468369960785, 0.19489535689353943, 0.1948760449886322, 0.19485676288604736, 0.1948375254869461, 0.19481833279132843, 0.19479918479919434, 0.19478009641170502, 0.1947609931230545, 0.19474194943904877, 0.1947229653596878, 0.19470398128032684, 0.19468507170677185, 0.19466619193553925, 0.19464734196662903, 0.1946285367012024, 0.19460977613925934, 0.19459103047847748, 0.1945723295211792, 0.1945536732673645, 0.1945350468158722, 0.19451647996902466, 0.19449792802333832, 0.19447942078113556, 0.1944609433412552, 0.1944425255060196, 0.1944241225719452, 0.19440577924251556, 0.19438743591308594, 0.1943691521883011, 0.19435091316699982, 0.19433268904685974, 0.19431449472904205, 0.19429637491703033, 0.19427825510501862, 0.19426019489765167, 0.19424214959144592, 0.19422414898872375, 0.19420620799064636, 0.19418826699256897, 0.19417038559913635, 0.19415253400802612, 0.19413472712039948, 0.19411692023277283, 0.19409918785095215, 0.19408147037029266, 0.19406378269195557, 0.19404613971710205, 0.19402852654457092, 0.19401095807552338, 0.19399340450763702, 0.19397592544555664, 0.19395843148231506, 0.19394101202487946, 0.19392359256744385, 0.19390621781349182, 0.19388888776302338, 0.1938716173171997, 0.19385434687137604, 0.19383710622787476, 0.19381989538669586, 0.19380272924900055, 0.19378560781478882, 0.19376851618289948, 0.19375143945217133, 0.19373440742492676, 0.19371740520000458, 0.19370044767856598, 0.19368350505828857, 0.19366662204265594, 0.1936497539281845, 0.19363291561603546, 0.19361612200737, 0.19359934329986572, 0.19358262419700623, 0.19356591999530792, 0.1935492604970932, 0.19353261590003967, 0.19351600110530853, 0.19349941611289978, 0.1934828758239746, 0.19346638023853302, 0.19344988465309143, 0.1934334635734558, 0.1934170424938202, 0.19340065121650696, 0.1933843046426773, 0.19336800277233124, 0.19335168600082397, 0.1933354139328003, 0.19331921637058258, 0.19330300390720367, 0.19328685104846954, 0.1932707279920578, 0.19325460493564606, 0.19323855638504028, 0.1932225078344345, 0.19320650398731232, 0.19319048523902893, 0.1931745707988739, 0.19315865635871887, 0.19314275681972504, 0.19312690198421478, 0.19311107695102692, 0.19309528172016144, 0.19307953119277954, 0.19306378066539764, 0.19304805994033813, 0.1930323839187622, 0.19301672279834747, 0.19300112128257751, 0.19298553466796875, 0.19296997785568237, 0.1929544359445572, 0.1929389387369156, 0.19292345643043518, 0.19290804862976074, 0.19289261102676392, 0.19287723302841187, 0.1928618848323822, 0.19284655153751373, 0.19283126294612885, 0.19281597435474396, 0.19280074536800385, 0.19278553128242493, 0.1927703619003296, 0.19275522232055664, 0.1927400827407837, 0.19272498786449432, 0.19270993769168854, 0.19269488751888275, 0.19267986714839935, 0.19266489148139954, 0.19264991581439972, 0.19263499975204468, 0.19262008368968964, 0.19260522723197937, 0.1925903856754303, 0.19257555902004242, 0.19256079196929932, 0.1925460249185562, 0.1925312876701355, 0.19251656532287598, 0.19250190258026123, 0.19248723983764648, 0.19247262179851532, 0.19245801866054535, 0.19244346022605896, 0.19242891669273376, 0.19241440296173096, 0.19239991903305054, 0.1923854500055313, 0.19237099587917328, 0.19235660135746002, 0.19234220683574677, 0.19232788681983948, 0.1923135370016098, 0.1922992467880249, 0.1922849416732788, 0.1922706961631775, 0.19225646555423737, 0.19224227964878082, 0.19222810864448547, 0.19221395254135132, 0.19219982624053955, 0.19218574464321136, 0.19217167794704437, 0.19215764105319977, 0.19214361906051636, 0.19212962687015533, 0.1921156495809555, 0.19210173189640045, 0.1920878142118454, 0.19207391142845154, 0.19206003844738007, 0.19204623997211456, 0.19203241169452667, 0.19201861321926117, 0.19200485944747925, 0.19199112057685852, 0.19197741150856018, 0.19196371734142303, 0.19195003807544708, 0.1919364035129547, 0.19192279875278473, 0.19190920889377594, 0.19189564883708954, 0.19188210368156433, 0.1918685883283615, 0.19185510277748108, 0.19184163212776184, 0.191828191280365, 0.19181478023529053, 0.19180139899253845, 0.19178801774978638, 0.1917746514081955, 0.19176135957241058, 0.19174805283546448, 0.19173480570316315, 0.19172152876853943, 0.1917082965373993, 0.19169509410858154, 0.19168193638324738, 0.1916687786579132, 0.19165563583374023, 0.19164255261421204, 0.19162945449352264, 0.19161640107631683, 0.19160336256027222, 0.1915903389453888, 0.19157736003398895, 0.1915643960237503, 0.19155143201351166, 0.1915385127067566, 0.1915256381034851, 0.19151276350021362, 0.19149990379810333, 0.19148705899715424, 0.19147427380084991, 0.1914614886045456, 0.19144873321056366, 0.19143599271774292, 0.19142328202724457, 0.1914105862379074, 0.19139790534973145, 0.19138525426387787, 0.19137263298034668, 0.19136004149913788, 0.19134746491909027, 0.19133490324020386, 0.19132235646247864, 0.191309854388237, 0.19129736721515656, 0.1912848949432373, 0.19127245247364044, 0.19126002490520477, 0.1912476122379303, 0.1912352293729782, 0.1912228763103485, 0.19121053814888, 0.1911982297897339, 0.19118590652942657, 0.19117364287376404, 0.1911613941192627, 0.19114917516708374, 0.19113695621490479, 0.19112475216388702, 0.19111260771751404, 0.19110046327114105, 0.19108834862709045, 0.19107624888420105, 0.19106414914131165, 0.19105209410190582, 0.1910400688648224, 0.19102804362773895, 0.1910160481929779, 0.19100405275821686, 0.19099211692810059, 0.19098018109798431, 0.19096828997135162, 0.19095636904239655, 0.19094450771808624, 0.19093264639377594, 0.19092081487178802, 0.1909090131521225, 0.19089721143245697, 0.19088543951511383, 0.19087369740009308, 0.19086197018623352, 0.19085028767585754, 0.19083857536315918, 0.1908269077539444, 0.190815269947052, 0.1908036470413208, 0.1907920241355896, 0.19078046083450317, 0.19076886773109436, 0.19075733423233032, 0.19074581563472748, 0.19073429703712463, 0.19072280824184418, 0.1907113641500473, 0.19069989025592804, 0.19068847596645355, 0.19067706167697906, 0.19066567718982697, 0.19065430760383606, 0.19064296782016754, 0.19063162803649902, 0.1906203180551529, 0.19060900807380676, 0.1905977725982666, 0.19058650732040405, 0.1905752718448639, 0.19056403636932373, 0.19055286049842834, 0.19054168462753296, 0.19053052365779877, 0.19051939249038696, 0.19050827622413635, 0.19049715995788574, 0.1904860883951187, 0.19047503173351288, 0.19046397507190704, 0.1904529631137848, 0.19044196605682373, 0.19043096899986267, 0.1904200166463852, 0.1904090791940689, 0.19039812684059143, 0.19038720428943634, 0.19037631154060364, 0.19036544859409332, 0.1903546005487442, 0.19034375250339508, 0.19033293426036835, 0.1903221309185028, 0.19031134247779846, 0.1903005689382553, 0.19028982520103455, 0.19027909636497498, 0.1902683973312378, 0.19025768339633942, 0.19024702906608582, 0.19023637473583221, 0.1902257353067398, 0.1902150809764862, 0.19020450115203857, 0.19019392132759094, 0.1901833564043045, 0.19017279148101807, 0.19016225636005402, 0.19015173614025116, 0.1901412457227707, 0.19013077020645142, 0.19012030959129333, 0.19010987877845764, 0.19009944796562195, 0.19008901715278625, 0.19007864594459534, 0.19006827473640442, 0.1900579035282135, 0.19004757702350616, 0.19003723561763763, 0.19002695381641388, 0.19001664221286774, 0.19000639021396637, 0.1899961233139038, 0.18998590111732483, 0.18997567892074585, 0.18996548652648926, 0.18995529413223267, 0.18994510173797607, 0.18993496894836426, 0.18992483615875244, 0.18991471827030182, 0.1899046003818512, 0.18989452719688416, 0.18988445401191711, 0.18987442553043365, 0.189864382147789, 0.18985435366630554, 0.18984434008598328, 0.1898343563079834, 0.1898244023323059, 0.18981444835662842, 0.18980452418327332, 0.18979458510875702, 0.1897847056388855, 0.18977481126785278, 0.18976493179798126, 0.18975508213043213, 0.1897452473640442, 0.18973541259765625, 0.1897256225347519, 0.18971581757068634, 0.18970605731010437, 0.1896962821483612, 0.18968655169010162, 0.18967682123184204, 0.18966712057590485, 0.18965743482112885, 0.18964774906635284, 0.18963807821273804, 0.1896284520626068, 0.1896188110113144, 0.18960919976234436, 0.18959960341453552, 0.18959002196788788, 0.18958045542240143, 0.18957088887691498, 0.18956135213375092, 0.18955184519290924, 0.18954232335090637, 0.1895328313112259, 0.1895233690738678, 0.1895139217376709, 0.189504474401474, 0.1894950568675995, 0.18948563933372498, 0.18947622179985046, 0.18946686387062073, 0.1894574910402298, 0.18944813311100006, 0.18943879008293152, 0.18942947685718536, 0.1894201636314392, 0.18941089510917664, 0.18940161168575287, 0.1893923282623291, 0.1893831044435501, 0.18937386572360992, 0.18936465680599213, 0.18935543298721313, 0.18934625387191772, 0.1893371045589447, 0.18932794034481049, 0.18931879103183746, 0.18930965662002563, 0.1893005669116974, 0.18929146230220795, 0.1892823725938797, 0.18927331268787384, 0.18926425278186798, 0.1892552226781845, 0.18924619257450104, 0.18923717737197876, 0.18922817707061768, 0.18921920657157898, 0.18921023607254028, 0.1892012655735016, 0.18919233977794647, 0.18918341398239136, 0.18917451798915863, 0.1891656219959259, 0.18915672600269318, 0.18914785981178284, 0.1891390085220337, 0.18913015723228455, 0.1891213357448578, 0.18911254405975342, 0.18910373747348785, 0.18909496068954468, 0.1890861839056015, 0.1890774518251419, 0.18906866014003754, 0.18905997276306152, 0.18905125558376312, 0.18904255330562592, 0.1890338808298111, 0.18902520835399628, 0.18901653587818146]\n\n\nHere I define two useful functions for plotting the decision boundary of my trained logistic regression model: plot_data and draw_line. These functions come from the Perceptron blog post with slight modifications.\n\nfrom matplotlib import pyplot as plt\n\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3 # when p_dim == 2\n    markers = ['o', '^']\n    targets = [0, 1]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 40,  c = y[ix]*2-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = 'seismic', vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 100)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x,y,**kwargs)\n\nUsing these two functions, I plot the data points generated by classification_data, as well as the decision boundary of the trained logistic regression model (the weight vector of the model after the training loop). The trained model’s decision boundary also looks correct, separating the non-linearly separable data quite well.\n\nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\nax.set_xlim([-2, 2.5])\nax.set_ylim([-2, 2.5])\n\nlast_loss = loss_vec[len(loss_vec)-1]\nax.set_title(f'Loss: {last_loss:.3f}')\nax.grid(True)\n\nplot_data(X, y, ax)\ndraw_line(torch.clone(LR.w), -2, 2.5, ax, color= 'black')\n\n\n\n\n\n\n\n\nIn the next visualization, I plot the evolution of the model’s loss over each iteration of the training loop. The loss of the model converges at the end of the training process; it seems that the loss converges to a value around 0.17. Again, the loss appears to be monotonically decreasing (every iteration’s loss is lower than the previous iteration’s), but it is hard to determine towards the end of the training loop.\n\nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\nax.grid(True)\nax.scatter(torch.arange(len(loss_vec)), loss_vec, s = 20, color = \"slategrey\")\nlabs = ax.set(xlabel = \"Gradient Descent Step #\", ylabel = \"Loss\", title = \"Logistic Regression Loss over Gradient Descent Steps\")\n\n\n\n\n\n\n\n\nTo confirm that the model’s loss is monotonically decreasing, I wrote a quick function isMonoDecrease that compares the loss vector to the loss vector sorted in decreasing order. If the vectors are identical, than the model’s loss is monotonically decreasing. As can be seen, my model’s loss decreases in this fashion.\n\ndef isMonoDecrease(v):\n    dec = []\n    dec.extend(v)\n    dec.sort(reverse = True)\n    if (v == dec):\n        return True\n    return False\n\nisMonoDecrease(loss_vec)\n\nTrue\n\n\n\n\nExperiment 2: Benefits of Momentum\nHypothesis: On the same data, gradient descent with momentum (e.g. \\(\\beta = 0.9\\)) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with \\(\\beta = 0\\)).\nUsing the same data as the past experiment, I instantiated 9 different logistic regression models in order to experiment with different values for the learning rate \\(\\alpha\\). I chose the values 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9; I then trained each logistic regression model on the data and used the \\(\\alpha\\) value to perform the optimization steps (I opted to run the optimization loop for 100 iterations, to compare the speed up to ‘vanilla’ gradient descent).\n\nloss_dict = {}\nfor i in range(1, 10):\n    LR = LogisticRegression()\n    opt = GradientDescentOptimizer(LR)\n    loss_dict[i] = []\n    for _ in range(100):\n        local_loss = LR.loss(X, y).item()\n        opt.step(X, y, alpha= i/10.0, beta= 0.9)\n        \n        loss_dict[i].append(local_loss)\n\nIn the visualization below, I plotted the evolution of loss of the logistic regression models using different \\(\\alpha\\) values for the optimization steps. As we can see especially well when considering the logistic regression models using \\(\\alpha\\) values of 0.3, 0.6, 0.7, 0.8, and 0.9, the models’ loss converge to values comparable to the one seen in the ‘vanilla’ gradient descent experiment (around 0.2). However, this is achieved in far fewer optimization steps; the model using ‘vanilla’ gradient descent achieved this loss in 100 optimization steps, while these models using a momentum of \\(\\beta = 0.9\\) took only around 10-30 optimization steps. While these plots confirm my hypothesis (that a similar convergence of loss can be achieved in fewer optimization steps than using ‘vanilla’ gradient descent), there is an issue for when the \\(\\alpha\\) value \\(= 0.1\\) that I cannot explain. When \\(\\alpha = 0.1\\), it seems that the loss is actually monotonically increasing with every optimization step.\n\nfig, axs = plt.subplots(3, 3, figsize = (16, 12))\n\nfor i in range(3):\n    for j in range(3):\n        axs[i, j].grid(True)\n        axs[i, j].plot(loss_dict[i*3+j+1], color = \"slategrey\")\n        axs[i, j].scatter(torch.arange(len(loss_dict[i*3+j+1])), loss_dict[i*3+j+1], s = 20, color = \"slategrey\")\n\n        labs = axs[i, j].set(ylabel = \"Loss\", title = f\"Alpha: {(i*3+j+1)/10:.1f}\")\n\n\n\n\n\n\n\n\nAs the model with an \\(\\alpha\\) value of 0.3 was the most continuous, I chose it to compare the evolution of loss of a logistic regression model using gradient descent with momentum against a model that did not use momentum. I plotted the loss of the model that did not use momentum in red, and in green I plotted the loss of the model that did. As can be seen, The model that uses momentum in its gradient descent converged to an acceptable loss value far faster than the model that did not use gradient descent with momentum. (Note: since the model that used gradient descent without momentum trained for 1500 iterations, I shrank the window of the visualization to only plot 200 iterations, so that the difference in loss evolution between the two approaches would be more apparent.)\n\nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\nax.grid(True)\nax.scatter(torch.arange(len(loss_vec)), loss_vec, s = 20, color = \"red\")\nax.plot(loss_dict[3], color = \"green\")\nax.scatter(torch.arange(len(loss_dict[3])), loss_dict[3], s = 20, color = \"green\")\nax.set(xlim= (0,200))\nlabs = ax.set(xlabel = \"Gradient Descent Step #\", ylabel = \"Loss\", title = \"Evolution of Loss of Model with Momentum vs. Model Without\")\n\n\n\n\n\n\n\n\n\n\nExperiment 3: Overfitting\nInvestigation: Generate some data where \\(p_{dim} &gt; n_{points}\\). Do this twice with the exact same parameters. Call the first dataset \\(X\\_train\\), \\(y\\_train\\) and the second dataset \\(X\\_test\\), \\(y\\_test\\). Then, do an experiment in which you fit a logistic regression model to the data \\(X\\_train\\), \\(y\\_train\\) and obtain 100% accuracy on this training data. What is the accuracy of the test data?\nBelow I generated classification data twice using the same parameters (\\(n_{points} = 5\\), \\(noise = 0.4\\), and \\(p_{dim} = 20\\)) and labeled them as X_train and y_train and X_test and y_test. I then fit a logistic regression model to the data X_train, y_train until the classification accuracy was 100%.\n\nfrom sklearn.metrics import accuracy_score\ntorch.manual_seed(2)\n\nX_train, y_train = classification_data(n_points= 5, noise = 0.4, p_dims= 20)\n\ntorch.manual_seed(5)\nX_test, y_test = classification_data(n_points= 5, noise = 0.4, p_dims= 20)\n\nX_train = X_train.type(torch.FloatTensor)\ny_train = y_train.type(torch.FloatTensor)\nX_test = X_test.type(torch.FloatTensor)\ny_test = y_test.type(torch.FloatTensor)\n\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\nloss = 1.0\nwhile loss &gt; 0.01:\n    loss = LR.loss(X_train, y_train).item()\n    opt.step(X_train, y_train, alpha= 0.01, beta= 0.2)\n    \n    loss_vec.append(loss)\n\npreds_train = LR.predict(X_train)\nprint(f'Accuracy on train data: {accuracy_score(y_train, preds_train)*100:.2f}%')\n\nAccuracy on train data: 100.00%\n\n\nBelow I have visualized the evolution of the loss function on this data where there are more dimensions than data points. Understandably, when the data of the classification model adhere to such conditions, it takes far more iterations for the model to find a separating hyperplane so that there is no loss.\n\nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\nax.grid(True)\n\nax.scatter(torch.arange(len(loss_vec)), loss_vec, s = 20, color = \"slategrey\")\nlabs = ax.set(xlabel = \"Gradient Descent Step #\", ylabel = \"Loss\", title = \"Evolution of Loss when Number of Features &gt; Number of Data Points\")\n\n\n\n\n\n\n\n\nI then used the trained model to make predictions based upon the testing data, wherein the accuracy was far lower, around 60%, indicating the model’s overfitting to the training data.\n\n#print(f'Loss on test data: {LR.loss(X_test, y_test)*100:.2f}%')\n\npreds_test = LR.predict(X_test)\nprint(f'Accuracy on test data: {accuracy_score(y_test, preds_test)*100:.2f}%')\n\nAccuracy on test data: 60.00%\n\n\n\n\n\nConclusion\nFor this blog post, I created an implementation for a logistic regression classification model, and performed three experiments in order to confirm the functionality of my implementation, and to confirm various characteristics of logistic regression models in my implementation. In the first experiment, I performed ‘vanilla’ gradient descent in the optimization step of my model, and observed the model’s weight vector converge to a seemingly correct decision boundary line. I then used gradient descent with momentum in the optimization step of my model, as well as vary the learning rate value for \\(\\alpha\\), and observed a substantial speed-up in loss minimization using this momentum. Finally, I performed an experiment where I created training and testing data that my model would be likely to overfit itself upon, and observed the results of a logistic regression model overfitting to the training data."
  },
  {
    "objectID": "posts/PenguinsPost/index.html",
    "href": "posts/PenguinsPost/index.html",
    "title": "Classifying Palmer’s Penguins",
    "section": "",
    "text": "In this blog post I am hoping to find features within the given Palmer Penguins data set that will allow a machine learning model to distinguish penguins of different species. The data set, collected by Dr. Kristen Gorman and the Palmer Station in Antarctica, contains physiological measurements from a sample set of penguins that belong to three different species: Chinstrap, Gentoo, and Adelie. At the end of my exploration and model training, I decided to train my model on the following three features: the Island on which the penguin was found, the penguin’s Culmen Length in millimeters, and the penguin’s Culmen Depth in millimeters. Trained on these three features, my linear regression model was able to predict the species of a given penguin in the data set with 100% accuracy.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head(20)\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n5\n41.1\n18.2\n192.0\n4050.0\n8.62264\n-26.60023\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n7\n34.6\n21.1\n198.0\n4400.0\n8.55583\n-25.22588\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n8\n52.8\n20.0\n205.0\n4550.0\n9.25177\n-24.69638\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n11\n47.2\n15.5\n215.0\n4975.0\n8.30817\n-26.21651\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n13\n46.1\n18.2\n178.0\n3250.0\n8.85664\n-24.55644\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n14\n34.0\n17.1\n185.0\n3400.0\n8.01485\n-26.69543\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n15\n49.7\n18.6\n195.0\n3600.0\n9.75486\n-24.31198\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n16\n41.1\n17.5\n190.0\n3900.0\n8.94365\n-26.06943\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n17\n58.0\n17.8\n181.0\n3700.0\n9.14382\n-24.57994\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n18\n46.9\n16.6\n192.0\n2700.0\n9.80589\n-24.73735\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n19\n45.7\n13.9\n214.0\n4400.0\n8.62870\n-26.60484\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n20\n42.1\n19.1\n195.0\n4000.0\n9.05736\n-25.81513\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n23\n49.1\n14.8\n220.0\n5150.0\n7.89744\n-26.63405\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n24\n48.1\n15.1\n209.0\n5500.0\n8.45738\n-26.22664\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n25\n42.9\n13.1\n215.0\n5000.0\n7.68528\n-25.39181\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse"
  },
  {
    "objectID": "posts/PenguinsPost/index.html#abstract",
    "href": "posts/PenguinsPost/index.html#abstract",
    "title": "Classifying Palmer’s Penguins",
    "section": "",
    "text": "In this blog post I am hoping to find features within the given Palmer Penguins data set that will allow a machine learning model to distinguish penguins of different species. The data set, collected by Dr. Kristen Gorman and the Palmer Station in Antarctica, contains physiological measurements from a sample set of penguins that belong to three different species: Chinstrap, Gentoo, and Adelie. At the end of my exploration and model training, I decided to train my model on the following three features: the Island on which the penguin was found, the penguin’s Culmen Length in millimeters, and the penguin’s Culmen Depth in millimeters. Trained on these three features, my linear regression model was able to predict the species of a given penguin in the data set with 100% accuracy.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head(20)\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n5\n41.1\n18.2\n192.0\n4050.0\n8.62264\n-26.60023\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n7\n34.6\n21.1\n198.0\n4400.0\n8.55583\n-25.22588\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n8\n52.8\n20.0\n205.0\n4550.0\n9.25177\n-24.69638\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n11\n47.2\n15.5\n215.0\n4975.0\n8.30817\n-26.21651\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n13\n46.1\n18.2\n178.0\n3250.0\n8.85664\n-24.55644\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n14\n34.0\n17.1\n185.0\n3400.0\n8.01485\n-26.69543\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n15\n49.7\n18.6\n195.0\n3600.0\n9.75486\n-24.31198\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n16\n41.1\n17.5\n190.0\n3900.0\n8.94365\n-26.06943\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n17\n58.0\n17.8\n181.0\n3700.0\n9.14382\n-24.57994\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n18\n46.9\n16.6\n192.0\n2700.0\n9.80589\n-24.73735\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n19\n45.7\n13.9\n214.0\n4400.0\n8.62870\n-26.60484\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n20\n42.1\n19.1\n195.0\n4000.0\n9.05736\n-25.81513\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n23\n49.1\n14.8\n220.0\n5150.0\n7.89744\n-26.63405\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n24\n48.1\n15.1\n209.0\n5500.0\n8.45738\n-26.22664\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n25\n42.9\n13.1\n215.0\n5000.0\n7.68528\n-25.39181\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse"
  },
  {
    "objectID": "posts/PenguinsPost/index.html#explore",
    "href": "posts/PenguinsPost/index.html#explore",
    "title": "Classifying Palmer’s Penguins",
    "section": "Explore:",
    "text": "Explore:\nIn this section, I will construct and discuss a few interesting data visualizations so that we can better understand the Palmer’s Penguins dataset and what features we may be looking for.\n\nplot1 = sns.scatterplot(data = X_train, x = 'Culmen Length (mm)', \n    y = 'Culmen Depth (mm)', hue = y_train, \n    style = y_train, palette = \"dark\")\nplot1.set_title(\"Culmen Depth vs. Culmen Length\")\nplot1.legend(title = \"Species\")\n\n\n\n\n\n\n\n\nThis scatterplot seems to indicate a heavy correlation between a penguin’s culmen depth and length, and the penguin’s species classification. One species seems to have short and moderately deep culmens (Species 0), another has moderately long and deep culmens (Species 1), and the last species has moderately long and shallow culmens (Species 2).\n\nplot2 = sns.scatterplot(\n    data = X_train, x = 'Culmen Depth (mm)', \n    y = 'Flipper Length (mm)', hue = y_train, \n    style = y_train, palette = \"dark\")\nplot2.set_title(\"Flipper Length vs. Culmen Depth\")\nplot2.legend(title = \"Species\")\nplot2\n\n\n\n\n\n\n\n\nPursuing the lead from the last scatter plot, I decided to plot culmen depth against flipper length, which I predicted would be species-specific, in that it is likely that different species vary noticeably in flipper length. As we saw in the last plot, Species 0 and 1 have similar culmen depths. Additionally, they also seem to have similar flipper lengths, and are therefore very closely plotted in this comparison. However, Species 2 is distinct from the other species by both flipper length and culmen depth, and are thus greatly removed from the other groups in the top left corner of this plot. This points to the fact that Species 2 generally has longer flippers and shallower culmens than the other species, so much so, that they are easily separable in this comparison.\n\nplot3 = sns.boxplot(\n        data = X_train, x = y_train, \n        y = 'Culmen Length (mm)')\nplot3.set_xlabel(\"Species\")\nplot3.set_title(\"Culmen Length Distribution Across Species\")\nplot3\n\n\n\n\n\n\n\n\nReturning to the culmen length variable, we can plot its distribution across species. We can see that this variable generally distinguishes Species 0 from the other two species, in that Species 0 has a shorter culmen than both other species, which have similar culmen lengths. This points to the idea that culmen length is a good feature to use to distinguish penguins of Species 0.\n\nrow1 = X_train.query(\"Island_Biscoe == True\").mean()\nrow2 = X_train.query(\"Island_Dream == True\").mean()\nrow3 = X_train.query(\"Island_Torgersen == True\").mean()\ntable1 = pd.DataFrame([row1, row2, row3])\ntable1\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n44.945600\n15.863200\n209.320000\n4702.000000\n8.394096\n-26.086192\n1.0\n0.0\n0.0\n1.0\n0.072000\n0.928000\n0.520000\n0.480000\n\n\n1\n44.527835\n18.307216\n193.628866\n3734.793814\n9.163175\n-25.075329\n0.0\n1.0\n0.0\n1.0\n0.123711\n0.876289\n0.525773\n0.474227\n\n\n2\n39.350000\n18.441176\n192.235294\n3727.941176\n8.844635\n-25.748135\n0.0\n0.0\n1.0\n1.0\n0.205882\n0.794118\n0.470588\n0.529412\n\n\n\n\n\n\n\nFinally, here is a summary table that groups penguins by their island, and analyzes the average quantitative features of the penguin whose data were collected on the different islands. On takeaway is that the penguins found on Island Torgersen have shorter culmens than those on the other islands. Another is that the penguins on Island Boscoe have shallower culmens than those found on the other islands. Additionally, penguins on Island Boscoe are larger (with greater flipper length and body mass). Through these comparisons, it looks like the island on which a penguin is found can be a good indicator of what species the penguin may be. This is because there seems to be a greater concentration of feature variance across islands, i.e. Boscoe has larger penguins with shallow culmens, and Torgersen has penguins with longer culmens. I predict that the best features to distinguish penguins will be Culmen Length, Culmen Depth, Island, and Flipper Length/Body Mass."
  },
  {
    "objectID": "posts/PenguinsPost/index.html#model",
    "href": "posts/PenguinsPost/index.html#model",
    "title": "Classifying Palmer’s Penguins",
    "section": "Model:",
    "text": "Model:\nIn this section, I will find three features of the data and train a model on these features which achieves a 100% testing accuracy.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\nfrom sklearn.model_selection import cross_val_score\n\nHere, I am testing every possible permutation of one qualitative feature and two quantitative features, and training a model on each permutation. I will use the model that shows the best average accuracy according to cross validation.\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\nLR = LogisticRegression(max_iter=10000)\n\nfeature_comparison = pd.DataFrame(columns=['Features', 'Accuracy'])\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    avg_score = np.mean(cross_val_score(LR, X_train[cols], y_train, cv = 5))\n    \n    # I'm trying to put all results into a table, and then sort by score\n    feature_comparison.loc[len(feature_comparison)] = {'Features' : cols, 'Accuracy' : avg_score}\n\n\nbest_features = feature_comparison.sort_values('Accuracy', ascending = False).head(5)\nbest_features\n\n\n\n\n\n\n\n\nFeatures\nAccuracy\n\n\n\n\n15\n[Island_Biscoe, Island_Dream, Island_Torgersen...\n0.988311\n\n\n17\n[Island_Biscoe, Island_Dream, Island_Torgersen...\n0.972624\n\n\n19\n[Island_Biscoe, Island_Dream, Island_Torgersen...\n0.964932\n\n\n26\n[Island_Biscoe, Island_Dream, Island_Torgersen...\n0.964857\n\n\n18\n[Island_Biscoe, Island_Dream, Island_Torgersen...\n0.964857\n\n\n\n\n\n\n\nThese are the features used by the most accurate model.\n\nbest_features['Features'].iloc[0]\n\n['Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen',\n 'Culmen Length (mm)',\n 'Culmen Depth (mm)']\n\n\n\nfit_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', \n            'Island_Dream', 'Island_Torgersen']\n\nThis is the training accuracy of the best model.\n\nLR.fit(X_train[fit_cols], y_train)\nLR.score(X_train[fit_cols], y_train)\n\n0.99609375"
  },
  {
    "objectID": "posts/PenguinsPost/index.html#evaluate",
    "href": "posts/PenguinsPost/index.html#evaluate",
    "title": "Classifying Palmer’s Penguins",
    "section": "Evaluate:",
    "text": "Evaluate:\nIn this section, I will show the decision boundaries of my model, split out by the qualitative feature.\nFirst, here is the model’s testing accuracy, it is 100% correct.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[fit_cols], y_test)\n\n1.0\n\n\n\nfrom matplotlib.patches import Patch\nfrom matplotlib import pyplot as plt\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[fit_cols], y_train)\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\ny_test_pred = LR.predict(X_test[fit_cols])\n\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]], dtype=int64)\n\n\nAccording to the confusion matrix, my model seems to have performed perfectly on the testing data, and produced no errors in the classification."
  },
  {
    "objectID": "posts/PenguinsPost/index.html#discussion",
    "href": "posts/PenguinsPost/index.html#discussion",
    "title": "Classifying Palmer’s Penguins",
    "section": "Discussion",
    "text": "Discussion\nI performed cross validation on logistic regression models trained on every combination of 3 qualitative/quantitative features found within the Palmer’s Penguins data set. The model that performed the best (to 99.6% training accuracy) was trained on the features Island, Culmen Length, and Culmen Depth, and performed with a 100% testing accuracy. I think that if I were to explore this assignment further, I would like to experiment with other classification models, to see if there are ways to classify this data set with comparable accuracy, but using fewer features."
  }
]